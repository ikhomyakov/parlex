//! Core parsing traits and actions.
//!
//! This module defines the traits and data structures used by SLR parsers generated by
//! the [`aslr`] tool in the [`parlex-gen`](https://crates.io/crates/parlex-gen) crate.
//! It introduces the [`Parser`] trait as the entry point for parsing streams of
//! tokens produced by a lexer. It also contains the [`ParserAction`] enum and
//! associated trait families that control reductions, state transitions and ambiguity
//! resolution. See the README for an overview.
//!
//! [`aslr`]: https://crates.io/crates/parlex-gen

use crate::{Lexer, Token};
use anyhow::{Error, Result, anyhow, bail};
use smartstring::alias::String;
use std::fmt::Debug;

/// Parser action used by the parsing automaton.
///
/// Represents the next operation the parser should perform, such as shifting,
/// reducing, accepting, etc. Parameterized by identifiers for parser states,
/// productions, and ambiguities.
///
/// # Type Parameters
/// - `S`: Parser state identifier type implementing [`ParserStateID`].
/// - `P`: Production identifier type implementing [`ParserProdID`].
/// - `A`: Ambiguity identifier type implementing [`ParserAmbigID`].
#[derive(Clone, Copy, Debug, PartialEq, Eq)]
pub enum ParserAction<S, P, A>
where
    S: ParserStateID,
    P: ParserProdID,
    A: ParserAmbigID,
{
    /// Indicates a parsing error.
    Error,

    /// Signals successful parsing (accepting the input).
    Accept,

    /// Shifts the next token and transitions to the given parser state.
    Shift(S),

    /// Reduces by the given production rule.
    Reduce(P),

    /// Handles an ambiguity represented by the given ambiguity ID.
    Ambig(A),

    /// Performs a `goto` transition to the given parser state.
    Goto(S),
}

/// A trait representing an identifier for a parser state.
///
/// Each parser state corresponds to a position in the parsing automaton.
/// The type implementing this trait is typically generated by
/// **parlex-gen**’s parser generator **ASLR**.
pub trait ParserStateID: Copy + Debug + Eq + Into<usize> {
    /// The total number of parser states.
    const COUNT: usize;
}

/// A trait representing an identifier for an ambiguity in the grammar.
///
/// Ambiguities occur when multiple parses are possible for the same input span.
/// The type implementing this trait is typically generated by
/// **parlex-gen**’s parser generator **ASLR**.
pub trait ParserAmbigID: Copy + Debug + Eq + Into<usize> {
    /// The total number of ambiguity identifiers.
    const COUNT: usize;
}

/// A trait representing an identifier for a grammar production rule.
///
/// A production defines how tokens and nonterminals combine to form higher-level
/// syntactic constructs. The type implementing this trait is typically generated by
/// **parlex-gen**’s parser generator **ASLR**.
pub trait ParserProdID: Copy + Debug + Eq + Into<usize> {
    /// The associated token identifier type.
    type TokenID: ParserTokenID;

    /// The total number of production rules.
    const COUNT: usize;

    /// Returns the label (name) of this production.
    fn label(&self) -> &'static str;

    /// Returns the token ID of the left-hand side (LHS) nonterminal.
    fn lhs_token_id(&self) -> Self::TokenID;

    /// Returns the number of symbols on the right-hand side (RHS) of the production.
    fn size(&self) -> usize;
}

/// A trait representing an identifier for a terminal or nonterminal token in the grammar.
///
/// Token IDs represent both grammar terminals and nonterminals used by the parser.
/// The type implementing this trait is typically generated by
/// **parlex-gen**’s parser generator **ASLR**.
pub trait ParserTokenID: Copy + Debug + Eq + Into<usize> {
    /// The number of nonterminal symbols in the grammar.
    const COUNT_NONTERMINALS: usize;

    /// The number of terminal symbols in the grammar.
    const COUNT_TERMINALS: usize;

    /// The total number of tokens: terminals + nonterminals + 1 (error).
    const COUNT: usize;

    /// Returns the label (name) of this token.
    fn label(&self) -> &'static str;
}

/// Helper alias for the parser’s [`ParserAction`] type.
type Action<P, U> = ParserAction<
    <<P as Parser<U>>::ParserData as ParserData>::StateID,
    <<P as Parser<U>>::ParserData as ParserData>::ProdID,
    <<P as Parser<U>>::ParserData as ParserData>::AmbigID,
>;

/// Defines the data and configuration used by a parser.
/// Provides access to parser states, productions, ambiguities, and lookup tables.
///
/// The struct implementing this trait is automatically generated by
/// **parlex-gen**’s parser generator **ASLR**.
pub trait ParserData {
    /// Identifier type for parser states.
    type StateID: ParserStateID;

    /// Identifier type for grammar ambiguities.
    type AmbigID: ParserAmbigID;

    /// Identifier type for grammar tokens (terminals and nonterminals).
    type TokenID: ParserTokenID;

    /// Identifier type for grammar productions.
    type ProdID: ParserProdID;

    /// Returns the starting parser state.
    fn start_state() -> Self::StateID;

    /// Looks up the parser action for the given state and input token.
    fn lookup(
        state_id: Self::StateID,
        token_id: Self::TokenID,
    ) -> ParserAction<Self::StateID, Self::ProdID, Self::AmbigID>;

    /// Looks up the ambiguity table entry for the given ambiguity ID.
    ///
    /// Returns a reference to a static two-element array representing the
    /// ambiguity record. Each entry corresponds to a possible parser action.
    ///
    /// One action is typically a **Shift** and the other a **Reduce**,
    /// forming a classic *shift/reduce conflict*. Such conflicts can often be resolved
    /// using operator precedence and associativity rules.
    ///
    /// In contrast, *reduce/reduce* conflicts are generally not resolvable and
    /// usually indicate that the grammar should be refactored.
    fn lookup_ambig(
        ambig_id: Self::AmbigID,
    ) -> &'static [ParserAction<Self::StateID, Self::ProdID, Self::AmbigID>; 2];
}

/// Core interface for a parser driver.
///
/// The struct implementing this trait coordinates parser state, interacts with the lexer,
/// resolves ambiguities, and applies reductions based on the parser tables. It also provides
/// utility methods for inspecting and manipulating the parse stack.

/// Core interface for a lexer driver.
///
/// Implement this trait to define custom parser behavior.  
/// A parser driver typically encapsulates user-defined state and logic,
/// which the parser calls back into during parsing.
///
/// The user should implement this trait for their driver type and then
/// pass a mutable reference to it when invoking [`Parser::try_next`] or
/// [`Parser::try_collect`].  
/// This allows the driver to maintain and update any user-specific data
/// needed during lexing.
pub trait ParserDriver {
    /// The token type consumed by the parser.
    /// Must be provided by the implementor.
    type Token: Token<TokenID = <Self::ParserData as ParserData>::TokenID>;

    /// The input stream type producing tokens for parsing (the lexer).
    type Input: TryNext<Item = Self::Token, Error = Error>;

    /// The parser data providing states, productions, tokens, and lookup tables.
    /// Must be provided by the implementor and be the struct generated by
    /// **parlex-gen**’s parser generator, **ASLR**.
    type ParserData: ParserData;

    /// Resolves an ambiguity reported by the parser (e.g., shift/reduce).
    /// Must be provided by the implementor.
    ///
    /// # Parameters
    /// - `user_data`: User data.
    /// - `ambig`:  The ambiguity ID (`AmbigID`).
    /// - `tok2`:   The lookahead token at the ambiguity point.
    ///
    /// # Returns
    /// The selected parser [`Action`] to disambiguate the current state.
    ///
    /// # Errors
    /// Returns an error if the ambiguity cannot be resolved consistently.
    fn resolve_ambiguity(
        &mut self,
        parser: &mut Parser,
        ambig: <Self::ParserData as ParserData>::AmbigID,
        tok2: &<Self::Lexer as Lexer<U>>::Token,
    ) -> Result<Action<Self, U>>;

    /// Performs a grammar reduction for the given production rule.
    /// Must be provided by the implementor.
    ///
    /// # Parameters
    /// - `user_data`: User data.
    /// - `prod`:  The production being reduced (`ProdID`).
    /// - `token`: The lookahead token (normally not used).
    ///
    /// # Errors
    /// Returns an error if the reduction fails.
    fn reduce(
        &mut self,
        parser: &mut Parser,
        prod_id: <Self::ParserData as ParserData>::ProdID,
        token: &<Self::Lexer as Lexer<U>>::Token,
    ) -> Result<()>;
}

/// Statistics collected during the parsing process.
///
/// Tracks basic metrics about parser activity and performance.
#[derive(Debug, Clone, Default)]
pub struct ParserStats {
    /// The total number of tokens processed by the parser.
    pub tokens: usize,

    /// The number of shift actions performed.
    pub shifts: usize,

    /// The number of reduce actions performed.
    pub reductions: usize,

    /// The number of ambiguities encountered.
    pub ambigs: usize,
}

/// Core parser execution context.
///
/// `ParserCtx` manages the runtime state of the parser, including its lexer,
/// parser states, token stack, and statistics. It is responsible for coordinating
/// parsing operations, maintaining the parser stack, and collecting performance metrics.
///
/// # Type Parameters
/// - `I`: The type, which implements the [`TryNext`] trait (the lexer)
/// - `D`: The parser data definition, which implements the [`ParserData`] trait.
///   This type is usually generated by the **parlex-gen** parser generator (**ASLR**).
/// - `T`: The token type consumed by the parser.

pub struct Parser<I, D>
where
    I: TryNext<Item = T, Error = Error>,
    D: ParserData,
    T: Token<TokenID = D::TokenID>,
{
    /// The input stream of tokens being processed by the parser.
    input: I,

    /// The stack of tokens currently held by the parser.
    tokens: Vec<T>,

    /// The stack of parser state identifiers.
    states: Vec<D::StateID>,

    /// Statistics collected during parsing.
    stats: ParserStats,
}

/// Implementation of [`Lexer`] methods.
///
/// This `impl` defines the primary construction and matching logic for the parser.
/// It provides methods to initialize a new parser instance, and perform SLR parsing
/// over the input stream.
///
/// # Type Parameters
/// - `I`: The input source (typically lexer), which must be a [`TryNext<Item = T, Error = anyhow::Error>`] yielding tokens.
/// - `D`: The parser definition type implementing the [`ParserData`] trait.
///   This type is usually generated by the **parlex-gen** parser generator (**ASLR**).
/// - `T`: The token type implementing the [`Token`] trait.
impl<I, D, T> Parser<I, D, T>
where
    I: TryNext<Item = T, Error = Error>,
    D: ParserData,
    T: Token<TokenID = D::TokenID>,
{
    /// Constructs a new [`ParserCtx`] from the given lexer.
    ///
    /// This initializes empty token and state stacks and resets parser statistics.
    ///
    /// # Parameters
    /// - `lexer`: The lexer instance used to produce tokens for this parser.
    ///
    /// # Returns
    /// A fully initialized parser context ready for use.
    pub fn new(input: I) -> Self {
        Self {
            input,
            tokens: Vec::new(),
            states: Vec::new(),
            stats: ParserStats::default(),
        }
    }

    /// Returns a reference to a token counted from the end of the stack.
    /// `0` = last (top), `1` = second last, etc.
    ///
    /// # Panics
    /// Panics if `index` ≥ the number of tokens on the stack.
    #[inline]
    fn tokens_peek<'a>(&'a self, index: usize) -> &'a T {
        let n = self.tokens.len();
        &self.tokens[n - 1 - index]
    }

    /// Returns a mutable reference to a token counted from the end of the stack.
    /// `0` = last (top), `1` = second last, etc.
    ///
    /// # Panics
    /// Panics if `index` ≥ the number of tokens on the stack.
    #[inline]
    fn tokens_mut_peek<'a>(&'a mut self, index: usize) -> &'a mut T {
        let n = self.tokens.len();
        &mut self.tokens[n - 1 - index]
    }

    /// Pops and returns the last (top) token from the stack.
    ///
    /// # Errors
    /// Returns an error if the stack is empty.
    #[inline]
    fn tokens_pop(&mut self) -> T {
        self.tokens.pop().ok_or_else(|| anyhow!("stack underflow"))
    }

    /// Pushes a token onto the stack.
    #[inline]
    fn tokens_push(&mut self, token: T) {
        self.tokens.push(token);
    }

    /// Traces the current parser state and token stack for debugging.
    /// Formats the stack with states and tokens, marking the incoming token with `<-`.
    fn dump_state(&self, incoming: &<Self::Lexer as Lexer<U>>::Token) {
        let mut output = String::new();
        if !self.states.is_empty() {
            for (i, (token, state)) in self
                .ctx()
                .tokens
                .iter()
                .chain(std::iter::once(incoming))
                .zip(self.ctx().states.iter())
                .enumerate()
            {
                output.push_str(&format!(
                    "<{:?}>  {}{:?}  ",
                    state,
                    if i == self.ctx().states.len() - 1 {
                        "<-  "
                    } else {
                        ""
                    },
                    token,
                ));
            }
            log::trace!("{}", output);
        } else {
            log::trace!("<>");
        }
    }

    /// Attempts to collect all tokens until exhaustion.
    fn try_collect(&mut self, user_data: &mut U) -> Result<Vec<<Self::Lexer as Lexer<U>>::Token>> {
        let mut ts = Vec::new();
        while let Some(t) = self.try_next(user_data)? {
            ts.push(t);
        }
        Ok(ts)
    }
}

impl<I, D, T> TryNext for Parser<I, D, T>
where
    I: TryNext<Item = T, Error = Error>,
    D: ParserData,
    T: Token<TokenID = D::TokenID>,
{
    /// Attempts to parse the input and produce the next reduced (accepted) token.
    #[inline]
    fn try_next(&mut self) -> Result<Option<T>> {
        self.states.clear();
        self.tokens.clear();
        self.stats.tokens += 1;
        let mut token = match self.lexer.try_next()? {
            Some(t) => t,
            None => {
                return Ok(None);
            }
        };
        let mut state = D::start_state();
        self.states.push(state);
        if log::log_enabled!(log::Level::Trace) {
            self.dump_state(&token);
        }
        loop {
            let action = match <Self as Parser<U>>::ParserData::lookup(state, token.token_id()) {
                Action::<Self, U>::Ambig(ambig) => {
                    log::trace!("Ambig {:?}", ambig);
                    let action = self.resolve_ambiguity(user_data, ambig, &mut token)?;
                    self.ctx_mut().stats.ambigs += 1;
                    action
                }
                action => action,
            };
            match action {
                Action::<Self, U>::Shift(new_state) => {
                    log::trace!("Shift {:?}", new_state);
                    self.ctx_mut().tokens.push(token);
                    state = new_state;
                    self.ctx_mut().states.push(state);
                    self.ctx_mut().stats.tokens += 1;
                    token = match self.ctx_mut().lexer.try_next(user_data)? {
                        Some(t) => t,
                        None => bail!("unexpected end of stream"),
                    };
                    self.ctx_mut().stats.shifts += 1;
                }

                Action::<Self, U>::Reduce(prod) => {
                    log::trace!("Reduce {:?}({})", prod, Into::<usize>::into(prod));
                    self.reduce(user_data, prod, &token)?;
                    let n = self.ctx().states.len() - prod.size();
                    self.ctx_mut().states.truncate(n);
                    state = self.ctx().states[self.ctx().states.len() - 1];
                    let lhs_id = self.ctx().tokens[self.ctx().tokens.len() - 1].token_id();
                    let Action::<Self, U>::Goto(new_state) =
                        <Self as Parser<U>>::ParserData::lookup(state, lhs_id)
                    else {
                        bail!("expected Action::Goto");
                    };
                    state = new_state;
                    self.ctx_mut().states.push(state);
                    self.ctx_mut().stats.reductions += 1;
                }

                Action::<Self, U>::Accept => {
                    log::trace!("Accept");
                    assert!(self.ctx().tokens.len() == 1);
                    let token = self.tokens_pop()?;
                    return Ok(Some(token));
                }

                Action::<Self, U>::Error => {
                    bail!("Error on token {:?}", token)
                }

                Action::<Self, U>::Ambig(_) | Action::<Self, U>::Goto(_) => unreachable!(),
            }

            if log::log_enabled!(log::Level::Trace) {
                self.dump_state(&token);
            }
        }
    }
}

/// Unit tests for [`ParserCtx`] and related components.
#[cfg(test)]
mod tests {
    use super::Action;
    use crate::lexer::{Lexer, LexerCtx, LexerData, LexerMode, LexerRule, LexerStats, Token};
    use crate::parser::{
        Parser, ParserAction, ParserAmbigID, ParserCtx, ParserData, ParserProdID, ParserStateID,
        ParserStats, ParserTokenID,
    };
    use anyhow::{Result, anyhow, bail};
    use regex_automata::PatternID;
    use smartstring::alias::String;
    use std::fmt::Debug;
    use std::iter::FusedIterator;

    /// Initializes the test logger to enable log output during tests.
    fn init_logger() {
        let _ = env_logger::builder().is_test(true).try_init();
    }

    #[derive(Debug, Clone, Copy)]
    struct XLexerMode;
    impl LexerMode for XLexerMode {
        const COUNT: usize = 0;
    }
    impl Into<usize> for XLexerMode {
        fn into(self) -> usize {
            0
        }
    }

    #[derive(Debug, Clone, Copy)]
    struct XLexerRule;
    impl LexerRule for XLexerRule {
        const COUNT: usize = 0;
        const END: Self = Self;
    }
    impl Into<usize> for XLexerRule {
        fn into(self) -> usize {
            0
        }
    }

    #[derive(Debug, Clone, Copy, PartialEq, Eq, Default)]
    struct TokenID(usize);

    impl From<TokenID> for usize {
        fn from(token_id: TokenID) -> Self {
            token_id.0
        }
    }

    impl ParserTokenID for TokenID {
        const COUNT_NONTERMINALS: usize = 5;
        const COUNT_TERMINALS: usize = 10;
        const COUNT: usize = 15;

        fn label(&self) -> &'static str {
            ""
        }
    }

    #[derive(Debug, Clone, Copy, PartialEq, Eq, Default)]
    struct StateID(usize);

    impl From<StateID> for usize {
        fn from(state_id: StateID) -> Self {
            state_id.0
        }
    }

    impl ParserStateID for StateID {
        const COUNT: usize = 45;
    }

    #[derive(Debug, Clone, Copy, PartialEq, Eq)]
    struct ProdID(usize);

    impl From<ProdID> for usize {
        fn from(prod_id: ProdID) -> Self {
            prod_id.0
        }
    }

    impl ParserProdID for ProdID {
        type TokenID = TokenID;

        const COUNT: usize = 24;

        fn label(&self) -> &'static str {
            ""
        }
        fn lhs_token_id(&self) -> Self::TokenID {
            TokenID(0)
        }
        fn size(&self) -> usize {
            0
        }
    }

    #[derive(Debug, Clone, Copy, PartialEq, Eq)]
    struct AmbigID(usize);

    impl From<AmbigID> for usize {
        fn from(token_id: AmbigID) -> Self {
            token_id.0
        }
    }

    impl ParserAmbigID for AmbigID {
        const COUNT: usize = 1500;
    }

    #[derive(Debug, Clone, Copy, Default)]
    struct XToken {
        token_id: TokenID,
        line_no: usize,
    }
    impl Token for XToken {
        type TokenID = TokenID;

        fn token_id(&self) -> Self::TokenID {
            self.token_id
        }
        fn line_no(&self) -> usize {
            self.line_no
        }
    }

    struct XLexerData {}
    impl LexerData for XLexerData {
        type LexerMode = XLexerMode;
        type LexerRule = XLexerRule;

        fn start_mode() -> Self::LexerMode {
            XLexerMode
        }
        fn dfa_bytes() -> &'static [u8] {
            &[]
        }

        #[inline]
        fn lookup(_mode: Self::LexerMode, _pattern_id: usize) -> Self::LexerRule {
            XLexerRule
        }
    }

    struct XLexer<I>
    where
        I: FusedIterator<Item = u8>,
    {
        ctx: LexerCtx<I, <Self as Lexer<()>>::LexerData, <Self as Lexer<()>>::Token>,
    }

    impl<I> XLexer<I>
    where
        I: FusedIterator<Item = u8>,
    {
        fn try_new(input: I) -> Result<Self> {
            let mut ctx = LexerCtx::try_new(input)?;
            ctx.end_flag = true;
            Ok(Self { ctx })
        }
    }

    impl<I> Lexer<()> for XLexer<I>
    where
        I: FusedIterator<Item = u8>,
    {
        type Input = I;
        type LexerData = XLexerData;
        type Token = XToken;

        fn ctx(&self) -> &LexerCtx<Self::Input, Self::LexerData, Self::Token> {
            &self.ctx
        }
        fn ctx_mut(&mut self) -> &mut LexerCtx<Self::Input, Self::LexerData, Self::Token> {
            &mut self.ctx
        }

        fn action(
            &mut self,
            user_data: &mut (),
            _rule: <Self::LexerData as LexerData>::LexerRule,
        ) -> Result<()> {
            self.yield_token(XToken {
                token_id: TokenID(0),
                line_no: 0,
            });
            Ok(())
        }
    }

    const AMBIG: &'static [ParserAction<StateID, ProdID, AmbigID>; 2] = &[
        ParserAction::Shift(StateID(0)),
        ParserAction::Reduce(ProdID(0)),
    ];

    struct XParserData {}
    impl ParserData for XParserData {
        type StateID = StateID;
        type AmbigID = AmbigID;
        type TokenID = TokenID;
        type ProdID = ProdID;

        fn start_state() -> Self::StateID {
            StateID::default()
        }

        fn lookup(
            state_id: Self::StateID,
            token_id: Self::TokenID,
        ) -> ParserAction<Self::StateID, Self::ProdID, Self::AmbigID> {
            ParserAction::Accept
        }

        fn lookup_ambig(
            ambig_id: Self::AmbigID,
        ) -> &'static [ParserAction<Self::StateID, Self::ProdID, Self::AmbigID>; 2] {
            AMBIG
        }
    }

    struct XParser<I>
    where
        I: FusedIterator<Item = u8>,
    {
        ctx: ParserCtx<XLexer<I>, <Self as Parser<()>>::ParserData, ()>,
    }

    impl<I> XParser<I>
    where
        I: FusedIterator<Item = u8>,
    {
        fn try_new(input: I) -> Result<Self> {
            let lexer = XLexer::try_new(input)?;
            let ctx = ParserCtx::new(lexer);
            Ok(Self { ctx })
        }
    }

    impl<I> Parser<()> for XParser<I>
    where
        I: FusedIterator<Item = u8>,
    {
        type Lexer = XLexer<I>;
        type ParserData = XParserData;

        fn ctx(&self) -> &ParserCtx<Self::Lexer, Self::ParserData, ()> {
            &self.ctx
        }
        fn ctx_mut(&mut self) -> &mut ParserCtx<Self::Lexer, Self::ParserData, ()> {
            &mut self.ctx
        }

        fn resolve_ambiguity(
            &mut self,
            user_data: &mut (),
            ambig: <Self::ParserData as ParserData>::AmbigID,
            tok2: &<Self::Lexer as Lexer<()>>::Token,
        ) -> Result<Action<Self, ()>> {
            Ok(Action::<Self, ()>::Shift(StateID::default()))
        }

        fn reduce(
            &mut self,
            user_data: &mut (),
            prod_id: <Self::ParserData as ParserData>::ProdID,
            token: &<Self::Lexer as Lexer<()>>::Token,
        ) -> Result<()> {
            Ok(())
        }
    }

    /// Tests that an empty parser can be created and run without errors.
    #[test]
    fn empty_parser() {
        init_logger();
        let s = "hello";
        let mut parser = XParser::try_new(s.bytes().fuse()).unwrap();
        while let Some(t) = parser.try_next(&mut ()).unwrap() {
            dbg!(t);
        }
    }
}
