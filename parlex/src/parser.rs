//! Core parsing traits and actions.
//!
//! This module defines the traits and data structures used by SLR parsers generated by
//! the [`aslr`] tool in the [`parlex-gen`](https://crates.io/crates/parlex-gen) crate.
//! It introduces the [`Parser`] trait as the entry point for parsing streams of
//! tokens produced by a lexer. It also contains the [`ParserAction`] enum and
//! associated trait families that control reductions, state transitions and ambiguity
//! resolution. See the README for an overview.
//!
//! [`aslr`]: https://crates.io/crates/parlex-gen

use crate::Token;
use smartstring::alias::String;
use std::fmt::Debug;
use thiserror::Error;
use try_next::TryNextWithContext;

/// Represents all possible errors that can occur during parsing.
///
/// The [`ParserError`] type is generic over three type parameters:
/// - `IE`: the **input error type**, representing failures originating from the
///   lexer.
/// - `DE`: the **driver error type**, representing failures produced by the
///   [`ParserDriver`] implementation.
/// - `T`: the **token type**, representing the fundamental input elements
///   consumed by the parser. Must implement [`Token`] and [`Debug`].
///
/// Each variant corresponds to a distinct failure mode. Most variants can be
/// automatically constructed through [`?`] propagation via their respective
/// `From` implementations.
///
/// [`ParserDriver`]: crate::ParserDriver
#[derive(Debug, Error)]
pub enum ParserError<IE, DE, T>
where
    T: Token + std::fmt::Debug,
{
    /// No parser driver was found.
    #[error("missing driver")]
    MissingDriver,

    /// Encountered an invalid or unexpected token.
    #[error("bad token {0:?}")]
    BadToken(T),

    /// A `goto` action was expected but not found.
    #[error("expected action goto")]
    ExpectedActionGoto,

    /// The parser encountered an unexpected end of input.
    #[error("unexpected end of stream")]
    UnexpectedEndOfStream,

    /// The input source produced an error.
    #[error("lexer stream error: {0}")]
    Lexer(IE),

    /// The parser driver produced an error.
    #[error("driver error: {0}")]
    Driver(DE),
}

/// Parser action used by the parsing automaton.
///
/// Represents the next operation the parser should perform, such as shifting,
/// reducing, accepting, etc. Parameterized by identifiers for parser states,
/// productions, and ambiguities.
///
/// # Type Parameters
/// - `S`: Parser state identifier type implementing [`ParserStateID`].
/// - `P`: Production identifier type implementing [`ParserProdID`].
/// - `A`: Ambiguity identifier type implementing [`ParserAmbigID`].
#[derive(Clone, Copy, Debug, PartialEq, Eq)]
pub enum ParserAction<S, P, A>
where
    S: ParserStateID,
    P: ParserProdID,
    A: ParserAmbigID,
{
    /// Indicates a parsing error.
    Error,

    /// Signals successful parsing (accepting the input).
    Accept,

    /// Shifts the next token and transitions to the given parser state.
    Shift(S),

    /// Reduces by the given production rule.
    Reduce(P),

    /// Handles an ambiguity represented by the given ambiguity ID.
    Ambig(A),

    /// Performs a `goto` transition to the given parser state.
    Goto(S),
}

/// A trait representing an identifier for a parser state.
///
/// Each parser state corresponds to a position in the parsing automaton.
/// The type implementing this trait is typically generated by
/// **parlex-gen**’s parser generator **ASLR**.
pub trait ParserStateID: Copy + Debug + Eq + Into<usize> {
    /// The total number of parser states.
    const COUNT: usize;
}

/// A trait representing an identifier for an ambiguity in the grammar.
///
/// Ambiguities occur when multiple parses are possible for the same input span.
/// The type implementing this trait is typically generated by
/// **parlex-gen**’s parser generator **ASLR**.
pub trait ParserAmbigID: Copy + Debug + Eq + Into<usize> {
    /// The total number of ambiguity identifiers.
    const COUNT: usize;
}

/// A trait representing an identifier for a grammar production rule.
///
/// A production defines how tokens and nonterminals combine to form higher-level
/// syntactic constructs. The type implementing this trait is typically generated by
/// **parlex-gen**’s parser generator **ASLR**.
pub trait ParserProdID: Copy + Debug + Eq + Into<usize> {
    /// The associated token identifier type.
    type TokenID: ParserTokenID;

    /// The total number of production rules.
    const COUNT: usize;

    /// Returns the label (name) of this production.
    fn label(&self) -> &'static str;

    /// Returns the token ID of the left-hand side (LHS) nonterminal.
    fn lhs_token_id(&self) -> Self::TokenID;

    /// Returns the number of symbols on the right-hand side (RHS) of the production.
    fn size(&self) -> usize;
}

/// A trait representing an identifier for a terminal or nonterminal token in the grammar.
///
/// Token IDs represent both grammar terminals and nonterminals used by the parser.
/// The type implementing this trait is typically generated by
/// **parlex-gen**’s parser generator **ASLR**.
pub trait ParserTokenID: Copy + Debug + Eq + Into<usize> {
    /// The number of nonterminal symbols in the grammar.
    const COUNT_NONTERMINALS: usize;

    /// The number of terminal symbols in the grammar.
    const COUNT_TERMINALS: usize;

    /// The total number of tokens: terminals + nonterminals + 1 (error).
    const COUNT: usize;

    /// Returns the label (name) of this token.
    fn label(&self) -> &'static str;
}

/// Defines the data and configuration used by a parser.
/// Provides access to parser states, productions, ambiguities, and lookup tables.
///
/// The struct implementing this trait is automatically generated by
/// **parlex-gen**’s parser generator **ASLR**.
pub trait ParserData {
    /// Identifier type for parser states.
    type StateID: ParserStateID;

    /// Identifier type for grammar productions.
    type ProdID: ParserProdID;

    /// Identifier type for grammar ambiguities.
    type AmbigID: ParserAmbigID;

    /// Identifier type for grammar tokens (terminals and nonterminals).
    type TokenID: ParserTokenID;

    /// Returns the starting parser state.
    fn start_state() -> Self::StateID;

    /// Looks up the parser action for the given state and input token.
    fn lookup(
        state_id: Self::StateID,
        token_id: Self::TokenID,
    ) -> ParserAction<Self::StateID, Self::ProdID, Self::AmbigID>;

    /// Looks up the ambiguity table entry for the given ambiguity ID.
    ///
    /// Returns a reference to a static two-element array representing the
    /// ambiguity record. Each entry corresponds to a possible parser action.
    ///
    /// One action is typically a **Shift** and the other a **Reduce**,
    /// forming a classic *shift/reduce conflict*. Such conflicts can often be resolved
    /// using operator precedence and associativity rules.
    ///
    /// In contrast, *reduce/reduce* conflicts are generally not resolvable and
    /// usually indicate that the grammar should be refactored.
    fn lookup_ambig(
        ambig_id: Self::AmbigID,
    ) -> &'static [ParserAction<Self::StateID, Self::ProdID, Self::AmbigID>; 2];
}

/// Core interface for a parser driver.
///
/// Implement this trait to define custom parsing behavior.
/// A parser driver encapsulates user-defined logic to handle conflict
/// (ambiguity) resolution and production rule reduction.
pub trait ParserDriver {
    /// The parser data providing states, productions, tokens, and lookup tables.
    /// Typically the struct generated by **parlex-gen**’s parser generator (**ASLR**).
    type ParserData: ParserData;

    /// The token type consumed by the parser.
    type Token: Token<TokenID = <Self::ParserData as ParserData>::TokenID>;

    /// The parser instance type.
    /// References the runtime parser object invoking this driver.
    type Parser;

    /// The user-defined context type.
    /// Used to store mutable state or application-specific data during parsing.
    type Context;

    /// The driver’s custom error type.
    type Error;

    /// Resolves an ambiguity reported by the parser (e.g., shift/reduce conflicts).
    ///
    /// Called when the parser encounters an ambiguous grammar situation that
    /// requires user-defined disambiguation.
    ///
    /// # Parameters
    /// - `parser`: Mutable reference to the active parser instance.
    /// - `context`: Mutable reference to the user context.
    /// - `ambig`: The ambiguity identifier (`AmbigID`).
    /// - `tok2`: The lookahead token at the ambiguity point.
    ///
    /// # Returns
    /// The chosen parser [`Action`] to resolve the ambiguity.
    ///
    /// # Errors
    /// Returns an error if the ambiguity cannot be resolved deterministically.
    fn resolve_ambiguity(
        &mut self,
        parser: &mut Self::Parser,
        context: &mut Self::Context,
        ambig: <Self::ParserData as ParserData>::AmbigID,
        tok2: &Self::Token,
    ) -> Result<Action<Self>, Self::Error>;

    /// Performs a grammar reduction for the given production rule.
    ///
    /// Called when the parser applies a production during a reduce step.
    ///
    /// # Parameters
    /// - `parser`: Mutable reference to the active parser instance.
    /// - `context`: Mutable reference to the user context.
    /// - `prod_id`: The production being reduced (`ProdID`).
    /// - `token`: The lookahead token at the point of reduction (usually unused).
    ///
    /// # Errors
    /// Returns an error if the reduction fails.
    fn reduce(
        &mut self,
        parser: &mut Self::Parser,
        context: &mut Self::Context,
        prod_id: <Self::ParserData as ParserData>::ProdID,
        token: &Self::Token,
    ) -> Result<(), Self::Error>;
}

/// Statistics collected during the parsing process.
///
/// Tracks basic metrics about parser activity and performance.
#[derive(Debug, Clone, Default)]
pub struct ParserStats {
    /// The total number of tokens processed by the parser.
    pub tokens: usize,

    /// The number of shift actions performed.
    pub shifts: usize,

    /// The number of reduce actions performed.
    pub reductions: usize,

    /// The number of ambiguities encountered.
    pub ambigs: usize,
}

/// Core parser implementation and execution engine.
///
/// The [`Parser`] drives the SLR automata, and interacts with the [`ParserDriver`]
/// to perform user-defined logic such as conflict (ambiguity) resolution and
/// production rule reduction.
/// It also maintains the runtime state of the parser, including its lexer,
/// parser stack, active states, and performance statistics.
///
/// # Type Parameters
///
/// - `I`: Input source that yields tokens via [`TryNextWithContext`] (the lexer).
/// - `D`: The [`ParserDriver`] implementation that provides user-defined logic
///    for conflict resolution and production reduction.
///
/// [`ParserDriver`]: crate::ParserDriver
/// [`TryNextWithContext`]: crate::TryNextWithContext
pub struct Parser<I, D, C>
where
    I: TryNextWithContext<C, Item = D::Token>,
    D: ParserDriver<Parser = Parser<I, D, C>>,
{
    /// The parser driver responsible for conflict (ambiguity) resolution and
    /// production rule reduction.
    /// Temporarily moved out of the parser during driver callbacks to satisfy
    /// Rust’s borrowing rules.
    driver: Option<Box<D>>,

    /// The input stream of tokens being processed by the parser.
    lexer: I,

    /// The stack of tokens currently held by the parser.
    tokens: Vec<D::Token>,

    /// The stack of parser state identifiers.
    states: Vec<<D::ParserData as ParserData>::StateID>,

    /// Statistics collected during parsing.
    stats: ParserStats,
}

/// Implementation of [`Parser`] methods.
///
/// This `impl` defines the core construction and execution logic for the
/// parser engine. It includes routines to create new parser instances,
/// drive automata transitions, and invoke the driver’s
/// callback to resolve conflicts and reduce productions.
///
/// [`Parser`]: crate::Parser
impl<I, D, C> Parser<I, D, C>
where
    I: TryNextWithContext<C, Item = D::Token>,
    D: ParserDriver<Parser = Parser<I, D, C>>,
{
    /// Constructs a new [`Parser`] from the given lexer and driver.
    ///
    /// # Parameters
    /// - `lexer`: The input stream providing tokens.
    /// - `driver`: The [`ParserDriver`] implementation responsible for handling
    ///   conflict resolution and production reductions.
    ///
    /// # Returns
    /// A fully initialized [`Parser`] ready to begin parsing.
    ///
    /// [`ParserDriver`]: crate::ParserDriver
    pub fn new(lexer: I, driver: D) -> Self {
        Self {
            driver: Some(Box::new(driver)),
            lexer,
            tokens: Vec::new(),
            states: Vec::new(),
            stats: ParserStats::default(),
        }
    }

    /// Returns a reference to a token from the top of the stack.
    ///
    /// The `index` is counted from the top:
    /// - `0` — the topmost (most recent) token
    /// - `1` — the token below the top, and so on
    ///
    /// # Panics
    /// Panics if `index` is greater than or equal to the number of tokens
    /// currently on the stack.
    #[inline]
    pub fn tokens_peek<'a>(&'a self, index: usize) -> &'a D::Token {
        let n = self.tokens.len();
        &self.tokens[n - 1 - index]
    }

    /// Returns a mutable reference to a token from the top of the stack.
    ///
    /// The `index` is counted from the top:
    /// - `0` — the topmost (most recent) token
    /// - `1` — the token below the top, and so on
    ///
    /// # Panics
    /// Panics if `index` is greater than or equal to the number of tokens
    /// currently on the stack.
    #[inline]
    pub fn tokens_mut_peek<'a>(&'a mut self, index: usize) -> &'a mut D::Token {
        let n = self.tokens.len();
        &mut self.tokens[n - 1 - index]
    }

    /// Pops and returns the last (top) token from the stack.
    ///
    /// # Panics
    /// Panics if the tokens stack is empty.
    #[inline]
    pub fn tokens_pop(&mut self) -> D::Token {
        self.tokens.pop().expect("stack underflow while popping")
    }

    /// Pushes a token onto the stack.
    #[inline]
    pub fn tokens_push(&mut self, token: D::Token) {
        self.tokens.push(token);
    }

    /// Traces the current parser state and token stack for debugging.
    /// Formats the stack with states and tokens, marking the incoming token with `<-`.
    pub fn dump_state(&self, incoming: &D::Token) {
        let mut output = String::new();
        if !self.states.is_empty() {
            for (i, (token, state)) in self
                .tokens
                .iter()
                .chain(std::iter::once(incoming))
                .zip(self.states.iter())
                .enumerate()
            {
                output.push_str(&format!(
                    "<{:?}>  {}{:?}  ",
                    state,
                    if i == self.states.len() - 1 {
                        "<-  "
                    } else {
                        ""
                    },
                    token,
                ));
            }
            log::trace!("{}", output);
        } else {
            log::trace!("<>");
        }
    }

    /// This helper temporarily takes ownership of the driver from the [`Parser`]
    /// to satisfy Rust’s borrowing rules, allowing the driver to receive a
    /// mutable reference to the parser during the callback without causing
    /// aliasing.
    ///
    /// The driver is removed from `self`, the callback is executed, and then
    /// the driver is reinserted once the callback completes. This ensures safe,
    /// exclusive access to both the lexer and the driver during driver methods.
    ///
    /// # Parameters
    /// - `context`: Mutable reference to the driver's context.
    /// - `ambig`:  The ambiguity ID (`AmbigID`).
    /// - `tok2`:   The lookahead token at the ambiguity point.
    ///
    /// # Returns
    /// The selected parser [`Action`] to disambiguate the current state.
    ///
    /// # Errors
    /// Returns an error if the ambiguity cannot be resolved consistently.
    ///
    /// [`Parser`]: crate::Parser
    fn resolve_ambiguity(
        &mut self,
        context: &mut D::Context,
        ambig: <D::ParserData as ParserData>::AmbigID,
        tok2: &D::Token,
    ) -> Result<Action<D>, ParserError<I::Error, D::Error, D::Token>> {
        let mut driver = self
            .driver
            .take()
            .ok_or_else(|| ParserError::MissingDriver)?;
        let action = driver
            .resolve_ambiguity(self, context, ambig, tok2)
            .map_err(ParserError::Driver)?;
        self.driver = Some(driver);
        Ok(action)
    }

    /// This helper temporarily takes ownership of the driver from the [`Parser`]
    /// to satisfy Rust’s borrowing rules, allowing the driver to receive a
    /// mutable reference to the parser during the callback without causing
    /// aliasing.
    ///
    /// The driver is removed from `self`, the callback is executed, and then
    /// the driver is reinserted once the callback completes. This ensures safe,
    /// exclusive access to both the lexer and the driver during driver methods.
    ///
    /// Performs a grammar reduction for the given production rule.
    ///
    /// # Parameters
    /// - `parser`: Parser
    /// - `context`: User context.
    /// - `prod`:  The production being reduced (`ProdID`).
    /// - `token`: The lookahead token (normally not used).
    ///
    /// # Errors
    /// Returns an error if the reduction fails.
    fn reduce(
        &mut self,
        context: &mut D::Context,
        prod_id: <D::ParserData as ParserData>::ProdID,
        token: &D::Token,
    ) -> Result<(), ParserError<I::Error, D::Error, D::Token>> {
        let mut driver = self
            .driver
            .take()
            .ok_or_else(|| ParserError::MissingDriver)?;
        driver
            .reduce(self, context, prod_id, token)
            .map_err(ParserError::Driver)?;
        self.driver = Some(driver);
        Ok(())
    }
}

pub type Action<D> = ParserAction<
    <<D as ParserDriver>::ParserData as ParserData>::StateID,
    <<D as ParserDriver>::ParserData as ParserData>::ProdID,
    <<D as ParserDriver>::ParserData as ParserData>::AmbigID,
>;

/// [`TryNextWithContext`] implementation for [`Parser`].
///
/// Provides context-aware advancement of the parser.
///
/// [`TryNextWithContext`]: crate::TryNextWithContext
impl<I, D, C> TryNextWithContext<C> for Parser<I, D, C>
where
    I: TryNextWithContext<C, Item = D::Token>,
    D: ParserDriver<Parser = Parser<I, D, C>, Context = C>,
{
    type Item = D::Token;
    type Error = ParserError<I::Error, D::Error, D::Token>;

    /// Attempts to parse the input and produce the next reduced (accepted) token.
    ///
    /// Returns `Ok(Some(token))`, `Ok(None)` at end of input, or an error.
    #[inline]
    fn try_next_with_context(
        &mut self,
        context: &mut C,
    ) -> Result<Option<Self::Item>, Self::Error> {
        self.states.clear();
        self.tokens.clear();
        self.stats.tokens += 1;
        let mut token = match self
            .lexer
            .try_next_with_context(context)
            .map_err(ParserError::Lexer)?
        {
            Some(t) => t,
            None => {
                return Ok(None);
            }
        };
        let mut state = <D::ParserData as ParserData>::start_state();
        self.states.push(state);
        if log::log_enabled!(log::Level::Trace) {
            self.dump_state(&token);
        }
        loop {
            let action = match <D::ParserData as ParserData>::lookup(state, token.token_id()) {
                Action::<D>::Ambig(ambig) => {
                    log::trace!("Ambig {:?}", ambig);
                    let action = self.resolve_ambiguity(context, ambig, &mut token)?;
                    self.stats.ambigs += 1;
                    action
                }
                action => action,
            };
            match action {
                Action::<D>::Shift(new_state) => {
                    log::trace!("Shift {:?}", new_state);
                    self.tokens.push(token);
                    state = new_state;
                    self.states.push(state);
                    self.stats.tokens += 1;
                    token = match self
                        .lexer
                        .try_next_with_context(context)
                        .map_err(ParserError::Lexer)?
                    {
                        Some(t) => t,
                        None => return Err(ParserError::UnexpectedEndOfStream),
                    };
                    self.stats.shifts += 1;
                }

                Action::<D>::Reduce(prod) => {
                    log::trace!("Reduce {:?}({})", prod, Into::<usize>::into(prod));
                    self.reduce(context, prod, &token)?;
                    let n = self.states.len() - prod.size();
                    self.states.truncate(n);
                    state = self.states[self.states.len() - 1];
                    let lhs_id = self.tokens[self.tokens.len() - 1].token_id();
                    let Action::<D>::Goto(new_state) =
                        <D::ParserData as ParserData>::lookup(state, lhs_id)
                    else {
                        return Err(ParserError::ExpectedActionGoto);
                    };
                    state = new_state;
                    self.states.push(state);
                    self.stats.reductions += 1;
                }

                Action::<D>::Accept => {
                    log::trace!("Accept");
                    assert!(self.tokens.len() == 1);
                    let token = self.tokens_pop();
                    return Ok(Some(token));
                }

                Action::<D>::Error => {
                    return Err(ParserError::BadToken(token));
                }

                Action::<D>::Ambig(_) | Action::<D>::Goto(_) => unreachable!(),
            }

            if log::log_enabled!(log::Level::Trace) {
                self.dump_state(&token);
            }
        }
    }
}

/// Unit tests for [`Parser`] and related components.
#[cfg(test)]
mod tests {
    use crate::lexer::{Lexer, LexerData, LexerDriver, LexerError, LexerMode, LexerRule, Token};
    use crate::parser::{
        Parser, ParserAction, ParserAmbigID, ParserData, ParserDriver, ParserError, ParserProdID,
        ParserStateID, ParserTokenID,
    };
    use smartstring::alias::String;
    use std::fmt::Debug;
    use try_next::TryNextWithContext;

    include!(concat!(
        env!("CARGO_MANIFEST_DIR"),
        "/src/test_lexer_data.rs"
    ));
    include!(concat!(
        env!("CARGO_MANIFEST_DIR"),
        "/src/test_parser_data.rs"
    ));

    /// Initializes the test logger to enable log output during tests.
    fn init_logger() {
        let _ = env_logger::builder().is_test(true).try_init();
    }

    #[derive(Debug, Clone)]
    struct XToken {
        token_id: TokenID,
        line_no: usize,
    }
    impl Token for XToken {
        type TokenID = TokenID;

        fn token_id(&self) -> Self::TokenID {
            self.token_id
        }
        fn line_no(&self) -> usize {
            self.line_no
        }
    }

    struct XLexerDriver<I> {
        _marker: std::marker::PhantomData<I>,
    }

    impl<I> LexerDriver for XLexerDriver<I>
    where
        I: TryNextWithContext<String, Item = u8>,
    {
        type LexerData = LexData;
        type Token = XToken;
        type Lexer = Lexer<I, Self, String>;
        type Error = std::convert::Infallible;
        type Context = String;

        fn action(
            &mut self,
            lexer: &mut Self::Lexer,
            context: &mut Self::Context,
            _rule: <Self::LexerData as LexerData>::LexerRule,
        ) -> Result<(), Self::Error> {
            lexer.yield_token(XToken {
                token_id: TokenID::End,
                line_no: 10,
            });
            context.push('e');
            Ok(())
        }
    }

    struct XLexer<I>
    where
        I: TryNextWithContext<String, Item = u8>,
    {
        lexer: Lexer<I, XLexerDriver<I>, String>,
    }

    impl<I> XLexer<I>
    where
        I: TryNextWithContext<String, Item = u8>,
    {
        fn try_new(
            input: I,
        ) -> Result<
            Self,
            LexerError<
                <I as TryNextWithContext<String>>::Error,
                <XLexerDriver<I> as LexerDriver>::Error,
            >,
        > {
            let driver = XLexerDriver {
                _marker: std::marker::PhantomData,
            };
            let lexer = Lexer::try_new(input, driver)?;
            Ok(Self { lexer })
        }
    }
    impl<I> TryNextWithContext<String> for XLexer<I>
    where
        I: TryNextWithContext<String, Item = u8>,
    {
        type Item = XToken;
        type Error = LexerError<
            <I as TryNextWithContext<String>>::Error,
            <XLexerDriver<I> as LexerDriver>::Error,
        >;

        fn try_next_with_context(
            &mut self,
            context: &mut String,
        ) -> Result<Option<XToken>, <Self as TryNextWithContext<String>>::Error> {
            context.push('a');
            self.lexer.try_next_with_context(context)
        }
    }

    struct Empty {}
    impl TryNextWithContext<String> for Empty {
        type Item = u8;
        type Error = std::convert::Infallible;
        fn try_next_with_context(
            &mut self,
            context: &mut String,
        ) -> Result<Option<Self::Item>, Self::Error> {
            context.push('E');
            Ok(None)
        }
    }

    struct XParserDriver<I> {
        _marker: std::marker::PhantomData<I>,
    }

    impl<I> ParserDriver for XParserDriver<I>
    where
        I: TryNextWithContext<String, Item = XToken>,
    {
        type ParserData = ParData;
        type Token = XToken;
        type Parser = Parser<I, Self, String>;
        type Error = std::convert::Infallible;
        type Context = String;

        fn resolve_ambiguity(
            &mut self,
            _parser: &mut Self::Parser,
            _context: &mut String,
            _ambig: <Self::ParserData as ParserData>::AmbigID,
            _tok2: &Self::Token,
        ) -> Result<ParserAction<StateID, ProdID, AmbigID>, Self::Error> {
            Ok(ParserAction::Shift(StateID(0)))
        }

        fn reduce(
            &mut self,
            parser: &mut Self::Parser,
            context: &mut String,
            prod_id: <Self::ParserData as ParserData>::ProdID,
            token: &Self::Token,
        ) -> Result<(), Self::Error> {
            match prod_id {
                ProdID::Start => {
                    // Accept - does not get reduced
                    unreachable!()
                }

                ProdID::Rule1 => {
                    // S ->
                    context.push('p');
                    parser.tokens_push(XToken {
                        token_id: TokenID::S,
                        line_no: token.line_no(),
                    });
                }
            }
            Ok(())
        }
    }

    struct XParser<I>
    where
        I: TryNextWithContext<String, Item = u8>,
    {
        parser: Parser<XLexer<I>, XParserDriver<XLexer<I>>, String>,
    }

    impl<I> XParser<I>
    where
        I: TryNextWithContext<String, Item = u8>,
    {
        fn try_new(
            input: I,
        ) -> Result<
            Self,
            ParserError<
                LexerError<
                    <I as TryNextWithContext<String>>::Error,
                    <XLexerDriver<I> as LexerDriver>::Error,
                >,
                <XParserDriver<XLexer<I>> as ParserDriver>::Error,
                XToken,
            >,
        > {
            let lexer = XLexer::try_new(input).map_err(ParserError::Lexer)?;
            let driver = XParserDriver {
                _marker: std::marker::PhantomData,
            };
            let parser = Parser::new(lexer, driver);
            Ok(Self { parser })
        }
    }
    impl<I> TryNextWithContext<String> for XParser<I>
    where
        I: TryNextWithContext<String, Item = u8>,
    {
        type Item = XToken;
        type Error = ParserError<
            LexerError<
                <I as TryNextWithContext<String>>::Error,
                <XLexerDriver<I> as LexerDriver>::Error,
            >,
            <XParserDriver<XLexer<I>> as ParserDriver>::Error,
            XToken,
        >;

        fn try_next_with_context(
            &mut self,
            context: &mut String,
        ) -> Result<Option<XToken>, <Self as TryNextWithContext<String>>::Error> {
            context.push('b');
            self.parser.try_next_with_context(context)
        }
    }

    /// Tests that an empty lexer can be created and run without errors.
    #[test]
    fn empty_parser() {
        init_logger();
        let mut context = String::new();
        let input = Empty {};
        let mut parser = XParser::try_new(input).unwrap();
        while let Some(t) = parser.try_next_with_context(&mut context).unwrap() {
            dbg!(&t);
            assert_eq!(t.token_id(), TokenID::S);
            assert_eq!(t.line_no(), 10);
        }
        assert_eq!(context, "baEepba");
    }
}
