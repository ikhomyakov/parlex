//! Lexer for Prolog-like terms with operator definitions.
//!
//! This module defines the [`TermLexer`] type, which tokenizes Prolog-style term input
//! and produces [`TermToken`] values used by the parser.
//!
//! The lexer is automatically generated by the **[`alex`]** tool (part of [`parlex-gen`])
//! and incorporates pattern actions for building atoms, numbers, strings, dates, and
//! structured terms. It supports nested parentheses, quoted names, and multi-line
//! comments.
//!
//! In addition, the lexer integrates operator definitions via [`OperDefs`], allowing
//! it to recognize operator symbols and return specialized tokens annotated with
//! operator table indices. These operator-aware tokens are later used by the parser
//! to resolve shift/reduce conflicts using precedence and associativity during term parsing.
//!
//! # Components
//! - [`TermLexer`]: The main lexer implementation.
//! - [`TermToken`]: Token type emitted by the lexer.
//! - [`OperDefs`]: Table of operator definitions used for lookup.
//!
//! # Code Generation
//! The actual DFA tables and lexer rules are generated at build time by
//! **[`alex`]**, included from `OUT_DIR` as `lexer_data.rs`.
//!
//! [`TermLexer`]: struct.TermLexer
//! [`TermToken`]: struct.TermToken
//! [`OperDefs`]: crate::oper::OperDefs
//! [`alex`]: https://crates.io/crates/parlex-gen

use crate::oper::{Assoc, Fixity, OperArg, OperDef, OperDefs};
use crate::parser::TokenID;
use anyhow::{Error, Result, anyhow, bail};
use arena_terms::{Arena, Term, View};
use chrono::{DateTime, FixedOffset, Utc};
use smartstring::alias::String;
use std::io::{self, BufReader, Read};
use std::iter::FusedIterator;
use std::mem;

/// Includes the generated lexer definition produced by **`parlex-gen`**’s
/// [`alex`](https://crates.io/crates/parlex-gen) tool.
///
/// The included file (`lexer_data.rs`) contains the DFA tables, mode definitions,
/// and rule implementations required for the [`TermLexer`]. It is generated at
/// build time by the project’s `build.rs` script.
include!(concat!(env!("OUT_DIR"), "/lexer_data.rs"));

/// Represents a generic runtime value emitted or used by the lexer.
///
/// [`Value`] encapsulates auxiliary data produced during lexing, such as parsed
/// terms or index into parser's terms stack.
///
/// # Variants
/// - [`Value::None`] — Indicates absence of a value.
/// - [`Value::Term`] — Wraps a parsed [`arena_terms::Term`] instance.
/// - [`Value::Index`] — Holds a numeric index
#[derive(Debug, Clone, Copy, Default)]
pub enum Value {
    #[default]
    None,
    Term(Term),
    Index(usize),
}

/// Macro that implements [`TryFrom<Value>`] for selected target types.
///
/// This macro provides convenient conversions from a generic [`Value`] into
/// concrete types such as [`Term`] or `usize`.
/// If the variant does not match the expected type, an error is returned.
macro_rules! impl_tryfrom_value {
    ( $( $Variant:ident => $ty:ty ),+ $(,)? ) => {
        $(
            impl ::core::convert::TryFrom<Value> for $ty {
                type Error = ::anyhow::Error;
                fn try_from(v: Value) -> ::anyhow::Result<Self> {
                    match v {
                        Value::$Variant(x) => Ok(x),
                        _ => ::anyhow::bail!(
                            "invalid value: expected {}",
                            stringify!($Variant),
                        ),
                    }
                }
            }
        )+
    };
}

// Implements TryFrom<Value> for Term and usize.
impl_tryfrom_value! {
    Term => Term,
    Index => usize,
}

// Implements TryFrom<Value> for Option<Term>
impl TryFrom<Value> for Option<Term> {
    type Error = Error;
    fn try_from(v: Value) -> Result<Self> {
        match v {
            Value::None => Ok(None),
            Value::Term(x) => Ok(Some(x)),
            _ => ::anyhow::bail!("invalid value: expected Term or None"),
        }
    }
}

/// A token produced by the [`TermLexer`] for Prolog-like term parsing.
///
/// Each [`TermToken`] encapsulates:
/// - the syntactic token kind (`token_id`),
/// - an associated semantic [`Value`],
/// - the line number on which it was recognized, and
/// - an optional operator-table index used for precedence and associativity lookup.
///
/// Tokens are emitted by the lexer and consumed by the parser during syntax analysis.
///
/// [`TermLexer`]: struct.TermLexer
/// [`Value`]: enum.Value
/// [`OperDefs`]: crate::oper::OperDefs
#[derive(Debug, Clone)]
pub struct TermToken {
    ///
    pub token_id: TokenID,
    /// The associated value (if any).
    pub value: Value,
    /// The line number where the token was recognized.
    pub line_no: usize,
    /// Optional operator definition index.
    pub op_tab_index: Option<usize>,
}

impl TermToken {
    /// Creates a new [`TermToken`] with the specified token ID, value, and line number.
    ///
    /// # Parameters
    /// - `token_id`: Token identifier from the lexer’s token table.
    /// - `value`: Optional value attached to the token.
    /// - `line_no`: Source line number where this token was found.
    ///
    /// The `op_tab_index` field is initialized to `None` by default.
    #[must_use]
    pub fn new(token_id: TokenID, value: Value, line_no: usize) -> Self {
        Self {
            token_id,
            value,
            line_no,
            op_tab_index: None,
        }
    }
}

/// Implements the [`Token`] trait for [`TermToken`], allowing integration
/// with the `parlex` runtime parser library.
///
/// This provides access to token identifiers and source line tracking.
impl Token for TermToken {
    type TokenID = TokenID;

    fn token_id(&self) -> Self::TokenID {
        self.token_id
    }
    fn line_no(&self) -> usize {
        self.line_no
    }
}

/// Parses a date/time string into a Unix epoch timestamp in milliseconds.
///
/// Accepts either:
/// - an RFC 3339–formatted string (e.g., `"2024-03-15T10:30:00Z"`), or
/// - a custom date-time format pattern when `fmt` is provided.
///
/// The parsed value is normalized to UTC before conversion.
///
/// # Parameters
/// - `s`: The date-time string to parse.
/// - `fmt`: Optional custom format string compatible with [`chrono::DateTime::parse_from_str`].
///
/// # Returns
/// The corresponding timestamp in **milliseconds since the Unix epoch**.
///
/// # Errors
/// Returns an error if the input cannot be parsed according to the expected format.
///
/// [`chrono::DateTime::parse_from_str`]: chrono::DateTime::parse_from_str
fn parse_date_to_epoch(s: &str, fmt: Option<&str>) -> Result<i64> {
    let dt_fixed: DateTime<FixedOffset> = match fmt {
        None => DateTime::parse_from_rfc3339(s)?,
        Some(layout) => DateTime::parse_from_str(s, layout)?,
    };
    let dt_utc = dt_fixed.with_timezone(&Utc);
    Ok(dt_utc.timestamp_millis())
}

/// Parses a signed 64-bit integer from a string using the specified numeric base.
///
/// Supports standard bases (2, 8, 10, 16, etc.) via [`i64::from_str_radix`].
/// Returns `0` for empty strings.
///
/// # Parameters
/// - `s`: The numeric string slice.
/// - `base`: The integer base (radix), e.g., 10 for decimal or 16 for hexadecimal.
///
/// # Returns
/// The parsed integer as an `i64`.
///
/// # Errors
/// Returns an error if:
/// - the string contains invalid digits for the given base, or
/// - the parsed value exceeds the bounds of an `i64`.
fn parse_i64(s: &str, base: u32) -> Result<i64> {
    if s.is_empty() {
        return Ok(0);
    }
    match i64::from_str_radix(s, base) {
        Ok(n) => Ok(n.try_into()?),
        Err(e) if e.kind() == &std::num::IntErrorKind::InvalidDigit => {
            bail!("digit not valid for base")
        }
        Err(_) => bail!("number overflowed u64"),
    }
}

/// The main lexer for Prolog-like terms.
///
/// `TermLexer` tokenizes input streams into [`TermToken`]s using DFA tables
/// generated by **parlex-gen**’s [`alex`] tool.
/// It maintains lexer state, manages nested constructs, and recognizes operators
/// defined in [`OperDefs`].
///
/// # Type Parameters
/// - `I`: The input source implementing [`FusedIterator`] over bytes.
///
/// [`TermToken`]: crate::lexer::TermToken
/// [`OperDefs`]: crate::oper::OperDefs
/// [`alex`]: https://crates.io/crates/parlex-gen
pub struct TermLexer<I>
where
    I: FusedIterator<Item = u8>,
{
    /// The internal lexer execution context that manages DFA traversal,
    /// input buffering, token emission, and line tracking.
    ///
    /// This field implements [`LexerCtx`] from the **parlex** runtime library,
    /// providing the core state and control required for integrating this lexer
    /// with the **parlex** framework.
    ctx: LexerCtx<I, <Self as Lexer<Arena>>::LexerData, <Self as Lexer<Arena>>::Token>,

    /// The operator definition table used to resolve operator fixity,
    /// precedence, and associativity during lexing.
    pub opers: OperDefs,

    /// Nesting depth of parentheses `(...)` in the input.
    nest_count: isize,

    /// Nesting depth of block comments `/* ... */`.
    comment_nest_count: isize,

    /// Nesting depth of curly braces `{...}`.
    curly_nest_count: isize,

    /// Nesting depth inside embedded script sections (`{ ... }`).
    script_curly_nest_count: isize,

    /// Counter tracking progress when lexing binary data sections.
    bin_count: isize,

    /// Temporary buffer holding the label of the current
    /// binary section being processed.
    bin_label: Vec<u8>,

    /// Temporary buffer holding date/time parsing format
    ///  used for string-to-epoch conversions.
    date_format: String,
}

/// Implementation of [`TermLexer`] methods.
///
/// This `impl` provides core construction logic for initializing a new
/// term lexer instance. It prepares the internal [`LexerCtx`] state,
/// sets up operator definitions, and initializes all internal counters
/// used for managing nested structures and special tokens.
///
/// # Type Parameters
/// - `I`: The input source, which must implement [`FusedIterator`] over bytes.
impl<I> TermLexer<I>
where
    I: FusedIterator<Item = u8>,
{
    /// Constructs a new [`TermLexer`] from the given input stream.
    ///
    /// Initializes the internal [`LexerCtx`], loads operator definitions
    /// if provided, and resets all nesting and parsing counters.
    ///
    /// # Parameters
    /// - `input`: The input byte stream to be lexed.
    /// - `opers`: Optional operator definitions ([`OperDefs`]) used to
    ///   recognize operator tokens by fixity and precedence.
    ///   If `None`, an empty operator table is created.
    ///
    /// # Returns
    /// A ready-to-use [`TermLexer`] instance, or an error if the underlying
    /// [`LexerCtx`] initialization fails.
    ///
    /// # Errors
    /// Returns an error if DFA table deserialization in [`LexerCtx::try_new`]
    /// fails or the input cannot be processed.
    pub fn try_new(input: I, opers: Option<OperDefs>) -> Result<Self> {
        Ok(Self {
            ctx: LexerCtx::try_new(input)?,
            opers: match opers {
                Some(opers) => opers,
                None => OperDefs::new(),
            },
            nest_count: 0,
            comment_nest_count: 0,
            curly_nest_count: 0,
            script_curly_nest_count: 0,
            bin_count: 0,
            bin_label: Vec::new(),
            date_format: String::new(),
        })
    }

    /// Emits a token with the specified ID and no attached value.
    ///
    /// This is typically used for punctuation, delimiters, or other
    /// zero-value tokens. The emitted token includes the current line number
    /// from the [`LexerCtx`].
    fn yield_id(&mut self, token_id: TokenID) {
        //self.clear();
        self.yield_token(TermToken {
            token_id,
            value: Value::None,
            line_no: self.ctx().line_no,
            op_tab_index: None,
        });
    }

    /// Emits a token carrying an arena-allocated [`Term`] as its value.
    ///
    /// Used for tokens that encapsulate constructed terms, such as atoms,
    /// numbers, or compound expressions.
    fn yield_term(&mut self, token_id: TokenID, term: Term) {
        self.yield_token(TermToken {
            token_id,
            value: Value::Term(term),
            line_no: self.ctx().line_no,
            op_tab_index: None,
        });
    }

    /// Emits a token that carries an integer index as its value.
    ///
    /// Typically used for terms stack references in parser.
    fn yield_index(&mut self, token_id: TokenID, index: usize) {
        self.yield_token(TermToken {
            token_id,
            value: Value::Index(index),
            line_no: self.ctx().line_no,
            op_tab_index: None,
        });
    }

    /// Emits a token that includes both a term and an optional operator-table index.
    ///
    /// This is used for operator tokens that may correspond to multiple
    /// fixity definitions in the operator table, preserving their association
    /// for later parsing.
    fn yield_optab(&mut self, token_id: TokenID, term: Term, op_tab_index: Option<usize>) {
        self.yield_token(TermToken {
            token_id,
            value: Value::Term(term),
            line_no: self.ctx().line_no,
            op_tab_index,
        });
    }
}

/// Implements the [`Lexer`] trait for [`TermLexer`], integrating with the **parlex** runtime library.
///
/// This binding wires the generated DFA / rule set (`LexData`) to the concrete
/// term-lexing behavior provided by `TermLexer`. It exposes the lexer context
/// and defines the rule action callback that builds [`TermToken`]s.
///
/// # Associated Types
/// - `Input`      — The input byte iterator (must be [`FusedIterator<Item = u8>`]).
/// - `LexerData`  — The generated lexer tables and rule enums (`LexData`).
/// - `Token`      — The token type produced by this lexer (`TermToken`).
impl<I> Lexer<Arena> for TermLexer<I>
where
    I: FusedIterator<Item = u8>,
{
    type Input = I;
    type LexerData = LexData;
    type Token = TermToken;

    /// Returns a shared reference to the internal [`LexerCtx`].
    fn ctx(&self) -> &LexerCtx<Self::Input, Self::LexerData, Self::Token> {
        &self.ctx
    }

    /// Returns a mutable reference to the internal [`LexerCtx`].
    fn ctx_mut(&mut self) -> &mut LexerCtx<Self::Input, Self::LexerData, Self::Token> {
        &mut self.ctx
    }

    /// The primary user callback invoked by the **parlex** lexer for each matched rule.
    ///
    /// This method implements the term-specific lexing logic:
    /// - constructs arena-backed values (atoms, numbers, strings, dates, etc.),
    /// - tracks nesting (parentheses, braces, comments, script blocks),
    /// - consults [`OperDefs`] to recognize operators and annotate tokens with operator indices,
    /// - and emits tokens via the `yield_*` helpers.
    ///
    /// # Parameters
    /// - `arena`: The [`Arena`] used for allocating term values produced during lexing.
    /// - `rule`:  The matched lexer rule (from the generated [`LexerData::LexerRule`]).
    ///
    /// # Returns
    /// `Ok(())` on success; an error if token construction or state handling fails.
    ///
    /// # Errors
    /// Propagates errors from value parsing (e.g., numeric/date parsing),
    /// arena allocation, or invalid state transitions.
    fn action(
        &mut self,
        arena: &mut Arena,
        rule: <Self::LexerData as LexerData>::LexerRule,
    ) -> Result<()> {
        log::trace!(
            "ACTION begin: mode {:?}, rule {:?}, buf {:?}, buf2 {:?}, label {:?}, accum {}",
            self.ctx().mode,
            rule,
            str::from_utf8(&self.ctx().buffer),
            str::from_utf8(&self.ctx().buffer2),
            str::from_utf8(&self.bin_label),
            self.ctx().accum_flag,
        );
        match rule {
            Rule::Empty => {
                unreachable!()
            }
            Rule::LineComment => {}
            Rule::CommentStart => {
                if self.comment_nest_count == 0 {
                    self.begin(Mode::Comment);
                }
                self.comment_nest_count += 1;
            }
            Rule::CommentEnd => {
                self.comment_nest_count -= 1;
                if self.comment_nest_count == 0 {
                    self.begin(Mode::Expr);
                }
            }
            Rule::CommentChar | Rule::ExprSpace | Rule::CommentAnyChar => {}
            Rule::ExprNewLine | Rule::CommentNewLine => {
                self.ctx_mut().line_no += 1;
            }
            Rule::LeftParen => {
                self.nest_count += 1;
                self.yield_id(TokenID::LeftParen);
            }
            Rule::RightParen => {
                self.nest_count -= 1;
                self.yield_id(TokenID::RightParen);
            }
            Rule::LeftBrack => {
                self.nest_count += 1;
                self.yield_id(TokenID::LeftBrack);
            }
            Rule::RightBrack => {
                self.nest_count -= 1;
                self.yield_id(TokenID::RightBrack);
            }
            Rule::Comma => {
                self.yield_id(TokenID::Comma);
            }
            Rule::Pipe => {
                self.yield_id(TokenID::Pipe);
            }
            Rule::RightBrace => {
                self.nest_count -= 1;
                self.curly_nest_count -= 1;
                if self.curly_nest_count >= 0 {
                    self.begin(Mode::Str);
                    self.yield_id(TokenID::RightParen);
                    let op_tab_idx = self.opers.lookup("++");
                    self.yield_optab(TokenID::AtomOper, arena.atom("++"), op_tab_idx);
                    self.clear();
                    self.accum();
                } else {
                    self.yield_term(TokenID::Error, arena.str("}"));
                }
            }
            Rule::Func => {
                self.nest_count += 1;
                self.ctx_mut().buffer.pop();
                let s = self.take_str()?;
                let op_tab_idx = self.opers.lookup(&s);
                let op_tab = self.opers.get(op_tab_idx);

                let atom = arena.atom(s);

                if op_tab.is_oper() {
                    let (has_empty, has_non_empty) =
                        [Fixity::Prefix, Fixity::Infix, Fixity::Postfix]
                            .iter()
                            .filter_map(|f| {
                                op_tab
                                    .get_op_def(*f)
                                    .map(|x| x.args.len() <= OperDef::required_arity(*f))
                            })
                            .fold((false, false), |(e, ne), is_empty| {
                                if is_empty { (true, ne) } else { (e, true) }
                            });

                    match (has_empty, has_non_empty) {
                        (false, false) => unreachable!(),
                        (true, false) => {
                            self.yield_optab(TokenID::AtomOper, atom, op_tab_idx);
                            self.yield_id(TokenID::LeftParen);
                        }
                        (false, true) => {
                            self.yield_optab(TokenID::FuncOper, atom, op_tab_idx);
                        }
                        (true, true) => bail!("arguments conflict in op defs for {:?}", atom),
                    }
                } else {
                    self.yield_optab(TokenID::Func, atom, op_tab_idx);
                }
            }
            Rule::Var => {
                let s = self.take_str()?;
                self.yield_term(TokenID::Var, arena.var(s));
            }
            Rule::Atom => {
                if self.ctx().buffer == b"." && self.nest_count == 0 {
                    self.yield_id(TokenID::Dot);
                    self.yield_id(TokenID::End);
                } else {
                    let s = self.take_str()?;
                    let op_tab_idx = self.opers.lookup(&s);
                    let op_tab = self.opers.get(op_tab_idx);
                    let atom = arena.atom(s);
                    if op_tab.is_oper() {
                        self.yield_optab(TokenID::AtomOper, atom, op_tab_idx);
                    } else {
                        self.yield_optab(TokenID::Atom, atom, op_tab_idx);
                    }
                }
            }

            Rule::DateEpoch => {
                let mut s = self.take_str()?;
                s.pop();
                s.drain(0..5);
                let s = s.trim();
                let d = parse_i64(s, 10)?;
                self.yield_term(TokenID::Date, arena.date(d));
            }
            Rule::Date => {
                self.begin(Mode::Date);
                self.clear();
                self.ctx_mut().buffer2.clear();
                self.date_format.clear();
            }
            Rule::Date1 => {
                self.begin(Mode::Time);
                self.date_format.push_str("%Y-%m-%d");
                self.extend_buffer2_with_buffer();
            }
            Rule::Date2 => {
                self.begin(Mode::Time);
                self.date_format.push_str("%m/%d/%Y");
                self.extend_buffer2_with_buffer();
            }
            Rule::Date3 => {
                self.begin(Mode::Time);
                self.date_format.push_str("%d-%b-%Y");
                self.extend_buffer2_with_buffer();
            }
            Rule::Time1 => {
                self.begin(Mode::Zone);
                self.date_format.push_str("T%H:%M:%S%.f");
                self.extend_buffer2_with_buffer();
            }
            Rule::Time2 => {
                self.begin(Mode::Zone);
                self.date_format.push_str("T%H:%M:%S");
                self.extend_buffer2_with_buffer();
                self.ctx_mut().buffer2.extend(b":00");
            }
            Rule::Time3 => {
                self.begin(Mode::Zone);
                self.date_format.push_str(" %H:%M:%S%.f");
                self.extend_buffer2_with_buffer();
            }
            Rule::Time4 => {
                self.begin(Mode::Zone);
                self.date_format.push_str(" %H:%M:%S");
                self.extend_buffer2_with_buffer();
                self.ctx_mut().buffer2.extend(b":00");
            }
            Rule::Time5 => {
                self.begin(Mode::Zone);
                self.date_format.push_str(" %I:%M:%S%.f %p");
                self.extend_buffer2_with_buffer();
            }
            Rule::Time6 => {
                self.begin(Mode::Zone);
                self.date_format.push_str(" %I:%M:%S %p");
                let ctx = &mut self.ctx_mut();
                ctx.buffer2.extend(&ctx.buffer[..ctx.buffer.len() - 3]);
                ctx.buffer2.extend(b":00");
                ctx.buffer2.extend(&ctx.buffer[ctx.buffer.len() - 3..]);
            }
            Rule::Zone1 => {
                if self.ctx().mode == Mode::Time {
                    self.date_format.push_str(" %H:%M:%S");
                    self.ctx_mut().buffer2.extend(b" 00:00:00");
                }
                self.begin(Mode::Expr);
                self.date_format.push_str("%:z");
                self.ctx_mut().buffer2.extend(b"+00:00");
                let s = self.take_str2()?;
                let d = parse_date_to_epoch(s.trim_end(), Some(self.date_format.as_str()))?;
                self.yield_term(TokenID::Date, arena.date(d));
            }
            Rule::Zone2 => {
                if self.ctx().mode == Mode::Time {
                    self.date_format.push_str(" %H:%M:%S");
                    self.ctx_mut().buffer2.extend(b" 00:00:00");
                }
                self.begin(Mode::Expr);
                if self.ctx.buffer[0] == b' ' {
                    self.date_format.push(' ');
                }
                self.date_format.push_str("%:z");
                self.ctx_mut().buffer.pop();
                self.extend_buffer2_with_buffer();
                let s = self.take_str2()?;
                let d = parse_date_to_epoch(s.trim_end(), Some(self.date_format.as_str()))?;
                self.yield_term(TokenID::Date, arena.date(d));
            }
            Rule::TimeRightBrace => {
                self.begin(Mode::Expr);
                self.date_format.push_str(" %H:%M:%S%:z");
                self.ctx_mut().buffer2.extend(b" 00:00:00+00:00");
                let s = self.take_str2()?;
                let d = parse_date_to_epoch(&s, Some(self.date_format.as_str()))?;
                self.yield_term(TokenID::Date, arena.date(d));
            }
            Rule::ZoneRightBrace => {
                self.begin(Mode::Expr);
                self.date_format.push_str("%:z");
                self.ctx_mut().buffer2.extend(b"+00:00");
                let s = self.take_str2()?;
                let d = parse_date_to_epoch(&s, Some(self.date_format.as_str()))?;
                self.yield_term(TokenID::Date, arena.date(d));
            }

            Rule::Hex => {
                self.begin(Mode::Hex);
                self.ctx_mut().buffer2.clear();
            }
            Rule::HexSpace => {}
            Rule::HexNewLine => {
                self.ctx_mut().line_no += 1;
            }
            Rule::HexByte => {
                let s = str::from_utf8(&self.ctx().buffer)?;
                match u8::from_str_radix(s, 16) {
                    Ok(b) => {
                        self.ctx_mut().buffer2.push(b);
                    }
                    Err(_) => {
                        self.yield_term(TokenID::Error, arena.str(s));
                    }
                }
            }
            Rule::HexRightBrace => {
                self.ctx_mut().buffer.pop();
                let bytes = self.take_bytes2();
                self.yield_term(TokenID::Bin, arena.bin(bytes));
                self.begin(Mode::Expr);
            }
            Rule::Bin => {
                self.begin(Mode::Bin);
            }
            Rule::Text => {
                self.begin(Mode::Text);
            }
            Rule::BinSpace | Rule::TextSpace => {}
            Rule::BinNewLine | Rule::TextNewLine => {
                self.ctx_mut().line_no += 1;
            }
            r @ (Rule::BinCount | Rule::TextCount) => {
                let s = str::from_utf8(&self.ctx().buffer)?;
                let mut s = String::from(s.trim());
                if &s[s.len() - 1..] == "\n" {
                    self.ctx_mut().line_no += 1;
                }
                if &s[s.len() - 1..] == ":" {
                    s.pop();
                }
                self.bin_count = s.parse()?;
                if self.bin_count > 0 {
                    if r == Rule::BinCount {
                        self.begin(Mode::BinCount);
                    } else {
                        self.begin(Mode::TextCount);
                    }
                    self.clear();
                    self.accum();
                }
            }
            r @ (Rule::BinCountAnyChar | Rule::TextCountAnyChar) => {
                self.bin_count -= 1;
                if self.bin_count == 0 {
                    self.extend_buffer2_with_buffer();
                    self.clear();
                    if r == Rule::BinCountAnyChar {
                        self.begin(Mode::Bin);
                    } else {
                        self.begin(Mode::Text);
                    }
                }
            }
            r @ (Rule::BinCountNLChar | Rule::TextCountNewLine) => {
                self.ctx_mut().line_no += 1;
                if self.ctx_mut().buffer[0] == b'\r' {
                    self.ctx_mut().buffer.remove(0);
                }
                self.bin_count -= 1;
                if self.bin_count == 0 {
                    self.extend_buffer2_with_buffer();
                    self.clear();
                    if r == Rule::BinCountNLChar {
                        self.begin(Mode::Bin);
                    } else {
                        self.begin(Mode::Text);
                    }
                }
            }
            r @ (Rule::BinRightBrace | Rule::TextRightBrace) => {
                if r == Rule::BinRightBrace {
                    let bytes = self.take_bytes2();
                    self.yield_term(TokenID::Bin, arena.bin(bytes));
                } else {
                    let s = self.take_str2()?;
                    self.yield_term(TokenID::Str, arena.str(s));
                }
                self.begin(Mode::Expr);
            }
            r @ (Rule::BinLabelStart | Rule::TextLabelStart) => {
                self.bin_label.clear();
                let len = self.ctx().buffer.len();
                if self.ctx_mut().buffer[len - 1] == b'\n' {
                    self.ctx_mut().line_no += 1;
                    self.bin_label.push(b'\n');
                    self.ctx_mut().buffer.pop();
                    let len = self.ctx().buffer.len();
                    if self.ctx_mut().buffer[len - 1] == b'\r' {
                        self.bin_label.insert(0, b'\r');
                        self.ctx_mut().buffer.pop();
                    }
                } else {
                    let len = self.ctx().buffer.len();
                    let b = self.ctx().buffer[len - 1];
                    self.bin_label.push(b);
                    self.ctx_mut().buffer.pop();
                }

                let buf = mem::take(&mut self.ctx_mut().buffer);
                self.bin_label.extend(buf);

                if r == Rule::BinLabelStart {
                    self.begin(Mode::BinLabel);
                } else {
                    self.begin(Mode::TextLabel);
                }
            }
            r @ (Rule::BinLabelEnd | Rule::TextLabelEnd) => {
                if self.ctx_mut().buffer[0] != b':' {
                    self.ctx_mut().line_no += 1;
                }
                if self.ctx().buffer == self.bin_label {
                    if r == Rule::BinLabelEnd {
                        self.begin(Mode::Bin);
                    } else {
                        self.begin(Mode::Text);
                    }
                } else {
                    if r == Rule::TextLabelEnd && self.ctx_mut().buffer[0] == b'\r' {
                        self.ctx_mut().buffer.remove(0);
                    }
                    self.extend_buffer2_with_buffer();
                }
            }
            r @ (Rule::BinLabelNLChar | Rule::TextLabelNewLine) => {
                self.ctx_mut().line_no += 1;
                if r == Rule::TextLabelNewLine && self.ctx_mut().buffer[0] == b'\r' {
                    self.ctx_mut().buffer.remove(0);
                }
                self.extend_buffer2_with_buffer();
            }
            Rule::BinLabelAnyChar | Rule::TextLabelAnyChar => {
                self.extend_buffer2_with_buffer();
            }
            Rule::LeftBrace => {
                self.begin(Mode::Script);
                self.clear();
                self.accum();
            }
            Rule::ScriptNotBraces => {}
            Rule::ScriptLeftBrace => {
                self.script_curly_nest_count += 1;
            }
            Rule::ScriptRightBrace => {
                if self.script_curly_nest_count != 0 {
                    self.script_curly_nest_count -= 1;
                } else {
                    self.ctx_mut().buffer.pop();
                    let s = self.take_str()?;
                    self.yield_term(TokenID::Str, arena.str(s));
                    self.begin(Mode::Expr);
                }
            }
            Rule::ScriptNewLine => {
                self.ctx_mut().line_no += 1;
            }
            Rule::HexConst => {
                self.ctx_mut().buffer.drain(0..2);
                let s = self.take_str()?;
                let val = parse_i64(s.as_str(), 16)?;
                self.yield_term(TokenID::Int, arena.int(val));
            }
            Rule::BaseConst => {
                let s = self.take_str()?;
                let (base_str, digits) =
                    s.split_once('\'').ok_or(anyhow!("missing ' separator"))?;
                let base: u32 = base_str.parse().map_err(|_| anyhow!("invalid base"))?;
                let val = parse_i64(digits, base)?;
                self.yield_term(TokenID::Int, arena.int(val));
            }
            Rule::CharHex => {
                let mut s = self.take_str()?;
                s.drain(0..4);
                let val = parse_i64(s.as_str(), 16)?;
                self.yield_term(TokenID::Int, arena.int(val));
            }
            Rule::CharOct => {
                let mut s = self.take_str()?;
                s.drain(0..3);
                let val = parse_i64(s.as_str(), 8)?;
                self.yield_term(TokenID::Int, arena.int(val));
            }
            Rule::CharNewLine1 | Rule::CharNewLine2 | Rule::CharNewLine4 => {
                self.ctx_mut().line_no += 1;
                self.yield_term(TokenID::Int, arena.int('\n' as i64));
            }
            Rule::CharNotBackslash => {
                let mut s = self.take_str()?;
                s.drain(0..2);
                let val = s.chars().next().ok_or(anyhow!("invalid char"))? as i64;
                self.yield_term(TokenID::Int, arena.int(val));
            }
            Rule::CharCtrl => {
                let mut s = self.take_str()?;
                s.drain(0..4);
                let val = s.chars().next().ok_or(anyhow!("invalid char"))? as i64 - '@' as i64;
                self.yield_term(TokenID::Int, arena.int(val));
            }
            Rule::CharDel1 | Rule::CharDel2 => {
                self.yield_term(TokenID::Int, arena.int('\x7F' as i64));
            }
            Rule::CharEsc => {
                self.yield_term(TokenID::Int, arena.int('\x1B' as i64));
            }
            Rule::CharBell => {
                self.yield_term(TokenID::Int, arena.int('\u{0007}' as i64));
            }
            Rule::CharBackspace => {
                self.yield_term(TokenID::Int, arena.int('\u{0008}' as i64));
            }
            Rule::CharFormFeed => {
                self.yield_term(TokenID::Int, arena.int('\u{000C}' as i64));
            }
            Rule::CharNewLine3 => {
                self.yield_term(TokenID::Int, arena.int('\n' as i64));
            }
            Rule::CharCarriageReturn => {
                self.yield_term(TokenID::Int, arena.int('\r' as i64));
            }
            Rule::CharTab => {
                self.yield_term(TokenID::Int, arena.int('\t' as i64));
            }
            Rule::CharVerticalTab => {
                self.yield_term(TokenID::Int, arena.int('\u{000B}' as i64));
            }
            Rule::CharAny => {
                let mut s = self.take_str()?;
                s.drain(0..3);
                let val = s.chars().next().ok_or(anyhow!("invalid char"))? as i64;
                self.yield_term(TokenID::Int, arena.int(val));
            }
            Rule::OctConst => {
                let s = self.take_str()?;
                let val = parse_i64(s.as_str(), 8)?;
                self.yield_term(TokenID::Int, arena.int(val));
            }
            Rule::DecConst => {
                let s = self.take_str()?;
                let val = parse_i64(s.as_str(), 10)?;
                self.yield_term(TokenID::Int, arena.int(val));
            }
            Rule::FPConst => {
                let s = self.take_str()?;
                let val: f64 = s.parse()?;
                self.yield_term(TokenID::Real, arena.real(val));
            }
            Rule::DoubleQuote => {
                self.begin(Mode::Str);
                self.clear();
                self.accum();
            }
            Rule::SingleQuote => {
                self.begin(Mode::Atom);
                self.clear();
                self.accum();
            }
            Rule::StrAtomCharHex => {
                let len = self.ctx().buffer.len();
                let b: u8 = parse_i64(str::from_utf8(&self.ctx_mut().buffer[len - 2..])?, 16)?
                    .try_into()?;
                self.ctx_mut().buffer.truncate(len - 4);
                self.ctx_mut().buffer.push(b);
            }
            Rule::StrAtomCharOct => {
                let slash_pos = self.ctx().buffer.iter().rposition(|&b| b == b'\\').unwrap();
                let b: u8 = parse_i64(str::from_utf8(&self.ctx().buffer[slash_pos + 1..])?, 8)?
                    .try_into()?;
                self.ctx_mut().buffer.truncate(slash_pos);
                self.ctx_mut().buffer.push(b);
            }
            Rule::StrAtomCharCtrl => {
                let len = self.ctx().buffer.len();
                let b = self.ctx_mut().buffer[len - 1] - b'@';
                self.ctx_mut().buffer.truncate(len - 3);
                self.ctx_mut().buffer.push(b);
            }
            Rule::StrAtomCharDel1 => {
                let idx = self.ctx().buffer.len() - 2;
                self.ctx_mut().buffer.truncate(idx);
                self.ctx_mut().buffer.push(b'\x7F');
            }
            Rule::StrAtomCharDel2 => {
                let idx = self.ctx().buffer.len() - 3;
                self.ctx_mut().buffer.truncate(idx);
                self.ctx_mut().buffer.push(b'\x7F');
            }
            Rule::StrAtomCharEsc => {
                let idx = self.ctx().buffer.len() - 2;
                self.ctx_mut().buffer.truncate(idx);
                self.ctx_mut().buffer.push(b'\x1B');
            }
            Rule::StrAtomCharBell => {
                let idx = self.ctx().buffer.len() - 2;
                self.ctx_mut().buffer.truncate(idx);
                self.ctx_mut().buffer.push(b'\x07');
            }
            Rule::StrAtomCharBackspace => {
                let idx = self.ctx().buffer.len() - 2;
                self.ctx_mut().buffer.truncate(idx);
                self.ctx_mut().buffer.push(b'\x08');
            }
            Rule::StrAtomCharFormFeed => {
                let idx = self.ctx().buffer.len() - 2;
                self.ctx_mut().buffer.truncate(idx);
                self.ctx_mut().buffer.push(b'\x0C');
            }
            Rule::StrAtomCharNewLine => {
                let idx = self.ctx().buffer.len() - 2;
                self.ctx_mut().buffer.truncate(idx);
                self.ctx_mut().buffer.push(b'\n');
            }
            Rule::StrAtomCharCarriageReturn => {
                let idx = self.ctx().buffer.len() - 2;
                self.ctx_mut().buffer.truncate(idx);
                self.ctx_mut().buffer.push(b'\r');
            }
            Rule::StrAtomCharTab => {
                let idx = self.ctx().buffer.len() - 2;
                self.ctx_mut().buffer.truncate(idx);
                self.ctx_mut().buffer.push(b'\t');
            }
            Rule::StrAtomVerticalTab => {
                let idx = self.ctx().buffer.len() - 2;
                self.ctx_mut().buffer.truncate(idx);
                self.ctx_mut().buffer.push(b'\x0B');
            }
            Rule::StrAtomCharSkipNewLine => {
                self.ctx_mut().line_no += 1;
                self.ctx_mut().buffer.pop();
                let idx = self.ctx().buffer.len() - 1;
                if self.ctx_mut().buffer[idx] == b'\r' {
                    self.ctx_mut().buffer.pop();
                }
                self.ctx_mut().buffer.pop();
            }
            Rule::StrAtomCharAny | Rule::StrAtomCharBackslash => {
                let idx = self.ctx().buffer.len() - 2;
                self.ctx_mut().buffer.remove(idx);
            }
            Rule::StrChar | Rule::AtomChar | Rule::StrAtomCarriageReturn => {}
            Rule::StrDoubleQuote => {
                self.begin(Mode::Expr);
                self.ctx_mut().buffer.pop();
                let s = self.take_str()?;
                self.yield_term(TokenID::Str, arena.str(s));
            }
            Rule::AtomSingleQuote => {
                self.begin(Mode::Expr);
                self.ctx_mut().buffer.pop();
                let s = self.take_str()?;
                self.yield_term(TokenID::Atom, arena.atom(s));
            }
            Rule::AtomLeftParen => {
                self.begin(Mode::Expr);
                self.nest_count += 1;
                let mut s = self.take_str()?;
                s.truncate(s.len() - 2);
                self.yield_term(TokenID::Func, arena.atom(s));
            }
            Rule::AtomLeftBrace => {}
            Rule::StrLeftBrace => {
                self.begin(Mode::Expr);
                self.nest_count += 1;
                self.curly_nest_count += 1;
                let mut s = self.take_str()?;
                s.pop();
                self.yield_term(TokenID::Str, arena.str(s));
                let op_tab_idx = self.opers.lookup("++");
                self.yield_optab(TokenID::AtomOper, arena.atom("++"), op_tab_idx);
                self.yield_id(TokenID::LeftParen);
            }
            Rule::StrAtomNewLine => {
                self.ctx_mut().line_no += 1;
            }
            Rule::Error => {
                let s = self.take_str()?;
                self.yield_term(TokenID::Error, arena.str(s));
            }
            Rule::End => {
                if self.ctx().mode == Mode::Expr {
                    self.yield_id(TokenID::End);
                } else {
                    self.yield_term(TokenID::Error, arena.str("<END>"));
                }
            }
        }

        log::trace!(
            "ACTION end:   mode {:?}, rule {:?}, buf {:?}, buf2 {:?}, label {:?}, accum {}",
            self.ctx().mode,
            rule,
            str::from_utf8(&self.ctx().buffer),
            str::from_utf8(&self.ctx().buffer2),
            str::from_utf8(&self.bin_label),
            self.ctx().accum_flag,
        );

        Ok(())
    }
}

/// Unit tests for the [`TermLexer`] implementation.
#[cfg(test)]
mod tests {
    use super::*;

    fn lex(arena: &mut Arena, s: &str) -> Result<Vec<TermToken>> {
        let mut lx = TermLexer::try_new(s.bytes().fuse(), Some(OperDefs::new()))?;
        Ok(lx.try_collect(arena)?)
    }

    #[test]
    fn test_dates() {
        let _ = env_logger::builder().is_test(true).try_init();
        let mut arena = Arena::new();
        const DATES: &[(&str, u8)] = &[
            ("date{-5381856000000}", 0),
            ("date{-5381830320000}", 1),
            ("date{-5381830311000}", 2),
            ("date{-5381830310999}", 3),
            ("date{1799-06-16}", 0),
            ("date{1799-06-16Z}", 0),
            ("date{1799-06-16 Z}", 0),
            ("date{1799-06-16-00:00}", 0),
            ("date{1799-06-16 -00:00}", 0),
            ("date{1799-06-16T07:08}", 1),
            ("date{1799-06-16T07:08:09}", 2),
            ("date{1799-06-16T07:08:09Z}", 2),
            ("date{1799-06-16T07:08:09.001Z}", 3),
            ("date{1799-06-16T07:08:09 Z}", 2),
            ("date{1799-06-16T07:08:09.001 Z}", 3),
            ("date{1799-06-16T07:08:09+00:00}", 2),
            ("date{1799-06-16T07:08:09.001+00:00}", 3),
            ("date{1799-06-16T07:08:09 +00:00}", 2),
            ("date{1799-06-16T07:08:09.001 +00:00}", 3),
            ("date{1799-06-16T07:08:09Z}", 2),
            ("date{1799-06-16T07:08:09.001Z}", 3),
            ("date{1799-06-16 07:08:09 Z}", 2),
            ("date{1799-06-16T07:08:09.001 Z}", 3),
            ("date{1799-06-16 07:08:09+00:00}", 2),
            ("date{1799-06-16T07:08:09.001+00:00}", 3),
            ("date{1799-06-16 07:08:09 +00:00}", 2),
            ("date{1799-06-16 07:08:09.001 +00:00}", 3),
            ("date{1799-06-16T07:08Z}", 1),
            ("date{1799-06-16T07:08 Z  }", 1),
            ("date{  1799-06-16T07:08+00:00}", 1),
            ("date{ 1799-06-16T07:08 +00:00   }", 1),
            ("date{06/16/1799Z}", 0),
            ("date{06/16/1799 Z}", 0),
            ("date{06/16/1799+00:00}", 0),
            ("date{06/16/1799 +00:00}", 0),
            ("date{06/16/1799 07:08Z}", 1),
            ("date{06/16/1799 07:08:09Z}", 2),
            ("date{06/16/1799 07:08:09.001Z}", 3),
            ("date{06/16/1799 07:08 Z}", 1),
            ("date{06/16/1799 07:08:09 Z}", 2),
            ("date{06/16/1799 07:08:09.001 Z}", 3),
            ("date{06/16/1799 07:08+00:00}", 1),
            ("date{06/16/1799 07:08:09+00:00}", 2),
            ("date{06/16/1799 07:08:09.001+00:00}", 3),
            ("date{06/16/1799 07:08 +00:00}", 1),
            ("date{06/16/1799 07:08:09 +00:00}", 2),
            ("date{06/16/1799 07:08:09.001 +00:00}", 3),
            ("date{16-Jun-1799Z}", 0),
            ("date{16-jun-1799 Z}", 0),
            ("date{16-JUN-1799+00:00}", 0),
            ("date{16-Jun-1799 +00:00}", 0),
            ("date{16-Jun-1799 07:08Z}", 1),
            ("date{16-JUN-1799 07:08:09Z}", 2),
            ("date{16-Jun-1799 07:08:09.001Z}", 3),
            ("date{16-Jun-1799 07:08 Z}", 1),
            ("date{16-jun-1799 07:08:09 Z}", 2),
            ("date{16-Jun-1799 07:08:09.001 Z}", 3),
            ("date{16-Jun-1799 07:08+00:00}", 1),
            ("date{16-Jun-1799 07:08:09+00:00}", 2),
            ("date{16-Jun-1799 07:08:09.001+00:00}", 3),
            ("date{16-Jun-1799 07:08 +00:00}", 1),
            ("date{16-Jun-1799 07:08:09 +00:00}", 2),
            ("date{16-Jun-1799 07:08:09.001 +00:00}", 3),
        ];
        for (s, k) in DATES {
            let mut ts = lex(&mut arena, s).unwrap();
            let tok = ts.remove(0);
            assert_eq!(tok.token_id, TokenID::Date);
            let term = Term::try_from(tok.value).unwrap();
            let d = term.unpack_date(&arena).unwrap();
            assert_eq!(
                d,
                match k {
                    0 => -5381856000000,
                    1 => -5381830320000,
                    2 => -5381830311000,
                    3 => -5381830310999,
                    _ => unreachable!(),
                }
            );
        }
    }

    #[test]
    fn test_atoms() {
        let mut arena = Arena::new();
        let ts = lex(&mut arena, "\na+foo-x '^&%^&%^&%''abc' 'AAA'").unwrap();
        dbg!(&ts);
        assert!(ts.len() == 9);
        assert!(ts.iter().take(ts.len() - 1).all(|t| {
            t.line_no == 2
                && matches!(
                    Term::try_from(t.value.clone())
                        .unwrap()
                        .view(&arena)
                        .unwrap(),
                    View::Atom(_)
                )
        }));
    }

    #[test]
    fn test_bin() {
        let mut arena = Arena::new();
        let ts = lex(&mut arena, "% single line comment\nbin{3:\x00\x01\x02 eob:\x00\x01:aaa\x02:eob eob\n\x00\neob eob\r\n\x00\r\neob\r\n}\r\nhex{   0203 0405 FE }").unwrap();
        dbg!(&ts);
        assert!(ts.len() == 3);
        assert!(matches!(
            Term::try_from(ts[0].value.clone())
                .unwrap()
                .view(&arena)
                .unwrap(),
            View::Bin(_)
        ));
        match Term::try_from(ts[0].value.clone())
            .unwrap()
            .view(&arena)
            .unwrap()
        {
            View::Bin(bytes) => assert!(bytes == &[0, 1, 2, 0, 1, 58, 97, 97, 97, 2, 0, 0,]),
            _ => unreachable!(),
        }
    }

    #[test]
    fn test_text() {
        let mut arena = Arena::new();
        let ts = lex(&mut arena, "/* single /* line */ comment */\ntext{3:abc eob:de:aaa:eob eob\n0\neob eob\r\n1\r\neob\r\n}\r\n").unwrap();
        dbg!(&ts);
        assert!(ts.len() == 2);
        assert!(matches!(
            Term::try_from(ts[0].value.clone())
                .unwrap()
                .view(&arena)
                .unwrap(),
            View::Str(_)
        ));
        match Term::try_from(ts[0].value.clone())
            .unwrap()
            .view(&arena)
            .unwrap()
        {
            View::Str(s) => assert!(s == "abcde:aaa01"),
            _ => unreachable!(),
        }
    }

    #[test]
    fn test_texts() {
        let mut arena = Arena::new();
        let ts = lex(&mut arena, "/* single [ ( { /* line */ comment */\n\"hello\" {hello} text{5:hello} text{e:hello:e} text{e:h:e e:e:e 2:ll e:o:e} text{\ne\nhello\ne}").unwrap();
        dbg!(&ts);
        assert!(ts.len() == 7);
        assert!(matches!(
            Term::try_from(ts[0].value.clone())
                .unwrap()
                .view(&arena)
                .unwrap(),
            View::Str(_)
        ));
        assert!(ts.iter().take(ts.len() - 1).all(|t| {
            match Term::try_from(t.value.clone())
                .unwrap()
                .view(&arena)
                .unwrap()
            {
                View::Str(s) => s == "hello",
                _ => false,
            }
        }));
    }

    #[test]
    fn test_integers() {
        let mut arena = Arena::new();
        let ts = lex(&mut arena, "[2'01010001111, 10'123, 36'AZ]").unwrap();
        assert!(ts.len() == 8);
        assert!(matches!(ts[1].token_id, TokenID::Int));
    }

    #[test]
    fn lex_string_subs() {
        let _ = env_logger::builder().is_test(true).try_init();
        let arena = &mut Arena::new();
        let ts = lex(arena, "\"aaa{1 + 2}bbb{3 * 4}ccc\"").unwrap();
        assert_eq!(ts.len(), 18);
        let t0: Term = ts[0].value.clone().try_into().unwrap();
        let t1: Term = ts[8].value.clone().try_into().unwrap();
        let t2: Term = ts[16].value.clone().try_into().unwrap();
        assert_eq!(t0.unpack_str(arena).unwrap(), "aaa");
        assert_eq!(t1.unpack_str(arena).unwrap(), "bbb");
        assert_eq!(t2.unpack_str(arena).unwrap(), "ccc");
    }
}
