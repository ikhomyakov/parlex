# parlex-gen

[![Crates.io](https://img.shields.io/crates/v/parlex-gen.svg)](https://crates.io/crates/parlex-gen)
[![Documentation](https://docs.rs/parlex-gen/badge.svg)](https://docs.rs/parlex-gen)
[![License: LGPL-3.0-or-later](https://img.shields.io/badge/License-LGPL%203.0--or--later-blue.svg)](https://www.gnu.org/licenses/lgpl-3.0)
[![Rust](https://img.shields.io/badge/rust-stable-brightgreen.svg)](https://www.rust-lang.org)

Lexer and parser generator tools for Rust.

## Overview

`parlex-gen` provides two code generation tools:

- **`alex`**: Regex-based lexer generator for creating tokenizers from lexical specifications
- **`aslr`**: SLR parser generator for building parsers from grammar definitions

Both generators produce code that uses the [parlex](https://crates.io/crates/parlex) runtime library.

## Features

- Generate regex-based lexers with `alex`
- Generate SLR parsers with `aslr`

## Installation

Add this to your `Cargo.toml`:

```toml
[build-dependencies]
parlex-gen = "0.1"
```

You'll also need the runtime library:

```toml
[dependencies]
parlex = "0.1"
```

## Usage

### Lexer Generation with `alex`

Create a lexer specification file and use `alex` to generate a lexer:

```rust
// In your build.rs
use parlex_gen::alex;

fn main() {
    let manifest_dir = std::env::var("CARGO_MANIFEST_DIR").unwrap();
    let out_dir = PathBuf::from(std::env::var("OUT_DIR").unwrap());
    let input_file = PathBuf::from(&manifest_dir).join("src/lexer.alex");
    println!("cargo:rerun-if-changed={}", input_file.display());
    println!("cargo:warning=ALEX Input file is {}", input_file.display());
    println!(
        "cargo:warning=ALEX Output directory is {}",
        out_dir.display()
    );
    // Generate lexer from specification
    alex::generate(&input_file, &out_dir, "lexer_data", false).unwrap();
}
```


### Parser Generation with `aslr`

Create a grammar file and use `aslr` to generate an SLR parser:

```rust
// In your build.rs
use parlex_gen::aslr;

fn main() {
    let manifest_dir = std::env::var("CARGO_MANIFEST_DIR").unwrap();
    let input_file = PathBuf::from(&manifest_dir).join("src/parser.g");
    println!("cargo:rerun-if-changed={}", input_file.display());
    println!("cargo:warning=ASLR Input file is {}", input_file.display());
    println!(
        "cargo:warning=ASLR Output directory is {}",
        out_dir.display()
    );
    // Generate parser from specification
    aslr::generate(&input_file, &out_dir, "parser_data", false).unwrap();
}
```

Perfect — you’re exactly right.
In the **Alex** context, these aren’t *final tokens* yet — they’re **lexical rules** that describe how to *recognize the structural parts* of tokens (like prefixes, escapes, delimiters, etc.), rather than defining the full token grammar.

Here’s the revised, accurate general description — short, clear, and in Markdown style:

## Alex Lexer Specification Format

The **Alex** specification defines **lexical rules** for recognizing the textual structure of a language before parsing.
It describes how to match the **components of tokens** — such as identifiers, numbers, delimiters, operators, and string or block contents — using *regular expressions* and *lexical states*.

### Structure

An Alex specification typically contains:

1. **Macro definitions**
   Named regular expressions declared as:

   ```text
   NAME = regex
   ```

   Macros can be referenced with `{{NAME}}` inside other patterns.
   They are used to build complex rules from smaller reusable fragments (e.g., `{{DEC}}`, `{{ATOM}}`, `{{VAR}}`).

2. **Lexical rules**
   Each rule specifies **what pattern to match** and **in which lexical state** it applies:

   ```text
   RuleName: <State1, State2> pattern
   ```

   These rules describe low-level recognition of language elements — not yet semantic tokens, but the raw lexical building blocks.

3. **Lexical states**
   States define *contexts* that control which rules are active at any time.
   The lexer can switch states dynamically, allowing it to handle nested or context-dependent structures (for example, strings, comments, or embedded data blocks).


### Purpose

Alex specifications describe **how text should be broken down at the character and substring level**.
They capture fine-grained lexical structure — such as delimiters, number formats, escape sequences, and nested regions — that the parser later assembles into full syntax tokens.

This allows:

* **Context-sensitive lexing** through states
* **Composable regular expressions** through macros
* **Efficient, precompiled lexers** generated by `parlex-gen` for use in Rust

### Example

```text
WS        = [ \t\f]+
NL        = \r?\n
DEC       = [0-9]
ALPHA     = [A-Za-z_]
ALNUM     = [A-Za-z0-9_]
INT       = {{DEC}}+
ID        = {{ALPHA}}{{ALNUM}}*
OPER      = [=+\-*/();]

CommentStart: <Expr, Comment> /\*
CommentEnd: <Comment> \*/
CommentChar: <Comment> [^*\r\n]
CommentNewLine: <Comment> {{NL}}
SkipWS: <Expr> {{WS}}
NewLine: <Expr> {{NL}}
OperSym: <Expr> {{OPER}}
Ident: <Expr> {{ID}}
Integer: <Expr> {{INT}}
ErrorAny: <*> .
```


## ASLR Grammar Specification Format

An **ASLR specification** defines a context-free grammar for use with the `aslr` SLR(1) parser generator.
It consists of **production rules**, written in a simple, line-oriented format:

```
rule_name:   Nonterminal -> Symbol Symbol ...
```

* Each line defines **one production**.
* The **left-hand side (LHS)** is a nonterminal being defined.
* The **right-hand side (RHS)** lists terminals and/or nonterminals.
* An **empty RHS** represents an ε-production (the symbol can derive nothing).
* Multiple alternative productions for the same nonterminal are written as separate rules.
* Grammars can express nested and recursive definitions suitable for SLR(1) parsing.

### Naming Rules

* **Rule names** follow the pattern:
  `[a-z]([a-zA-Z0-9])*`
* **Nonterminals** use **capitalized names** (e.g., `Expr`, `Term`, `Seq`).
* **Terminals** follow either:

  * `[a-z]([a-zA-Z0-9])*` — for word-like tokens, or
  * one of the special symbols below, which are translated to lowercase names.

|                 |                 |                |                  |
| :-------------- | :-------------- | :------------- | :--------------- |
| `.` dot         | `-` minus       | `~` tilde      | `` ` `` backtick |
| `!` exclamation | `@` at          | `#` hash       | `$` dollar       |
| `%` percent     | `^` caret       | `&` ampersand  | `*` asterisk     |
| `+` plus        | `=` equals      | `\|` pipe      | `\\` backslash   |
| `<` lessThan    | `>` greaterThan | `?` question   | `/` slash        |
| `;` semicolon   | `(` leftParen   | `)` rightParen | `[` leftBrack    |
| `]` rightBrack  | `{` leftBrace   | `}` rightBrace | `,` comma        |
| `'` singleQuote | `"` doubleQuote | `:` colon      |                  |

### Example

```text
expr1:  Expr -> Expr + Expr
expr2:  Expr -> Expr * Expr
expr3:  Expr -> ( Expr )
expr4:  Expr -> int
```

## Documentation

For detailed API documentation and examples, visit [docs.rs/parlex-gen](https://docs.rs/parlex-gen).

## License

Copyright (c) 2005–2025 IKH Software, Inc.

Released under the terms of the GNU Lesser General Public License, version 3.0 or (at your option) any later version (LGPL-3.0-or-later).

## See Also

- [parlex](https://crates.io/crates/parlex) - Runtime support library
- [Main repository](https://github.com/ikhomyakov/parlex)
