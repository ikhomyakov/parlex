//! # Core Lexical Analysis Traits and Types
//!
//! This module defines the core traits and types used by lexers generated with the
//! [`alex`] generator from the [`parlex-gen`](https://crates.io/crates/parlex-gen) crate.
//!
//! It provides the [`Token`], [`LexerMode`], [`LexerRule`], [`LexerData`],
//! [`LexerDriver`], and [`LexerError`] traits, along with the [`Lexer`] type that
//! maintains scanning state.
//!
//! To implement a custom lexer, define a [`LexerDriver`] that orchestrates DFA
//! transitions, manages input buffering, and produces tokens using these core
//! abstractions.
//!
//! See the crate’s README for additional examples and implementation guidance.
//!
//! [`alex`]: https://crates.io/crates/parlex-gen
use regex_automata::{
    Anchored, HalfMatch, Input as RegexInput,
    dfa::{Automaton, dense},
    util::primitives::PatternID,
};
use smartstring::alias::String;
use std::collections::VecDeque;
use std::fmt::Debug;
use std::mem;
use thiserror::Error;
use try_next::TryNextWithContext;

/// Represents all possible errors that can occur during lexical analysis.
///
/// The [`LexerError`] type is generic over two error types:
/// - `IE`: the **input error type**, representing failures from the input source
///   (e.g., I/O or stream reading errors).
/// - `DE`: the **driver error type**, representing failures in the custom
///   [`LexerDriver`] implementation.
///
/// This enum provides comprehensive coverage for issues that can arise while
/// scanning input, performing DFA transitions, decoding UTF-8, or interacting
/// with the lexer driver.
///
/// Each variant captures a distinct failure mode, and most are automatically
/// constructed via [`?`] propagation through their respective `From`
/// implementations.
///
/// [`LexerDriver`]: crate::LexerDriver
#[derive(Debug, Error)]
pub enum LexerError<IE, DE> {
    /// No lexer driver was found.
    #[error("missing driver")]
    MissingDriver,

    /// Failed to deserialize DFA tables.
    ///
    /// Typically indicates corrupted or incompatible serialized data.
    #[error("failed to deserialize DFA tables: {0}")]
    DfaDeserialize(#[from] regex_automata::util::wire::DeserializeError),

    /// Failed to build DFA table.
    ///
    /// Currently used only in test mode.
    #[error("failed to deserialize DFA tables: {0}")]
    DfaBuild(#[from] dense::BuildError),

    /// A regex engine match error occurred during DFA evaluation.
    #[error("match error: {0}")]
    RegexMatch(#[from] regex_automata::MatchError),

    /// Attempted to pop beyond the buffer capacity (underflow).
    #[error("buffer underflow while popping (overpop)")]
    Overpop,

    /// Encountered an invalid or unexpected byte.
    #[error("bad byte {0:?}")]
    BadByte(u8),

    /// Failed to decode UTF-8 from input.
    #[error("utf8 error {0:?}")]
    FromUtf8(#[from] std::string::FromUtf8Error),

    /// The input source produced an error.
    #[error("input stream error: {0}")]
    Input(IE),

    /// The lexer driver produced an error.
    #[error("driver error: {0}")]
    Driver(DE),
}

/// A trait representing a token in lexical analysis or parsing.
///
/// Defines the minimal interface for objects treated as tokens, providing
/// a token identifier and its line position in the source.
pub trait Token: Clone + Debug {
    /// The type used to represent token identifiers.
    type TokenID: Into<usize>;

    /// Returns the identifier of this token.
    fn token_id(&self) -> Self::TokenID;

    /// Returns the line number where this token appears in the source.
    fn line_no(&self) -> usize;
}

/// A trait representing a lexer mode used during lexical analysis.
/// Each mode can be converted into an index and defines a total count of modes.
pub trait LexerMode: Copy + Debug + Into<usize> {
    /// Total number of lexer modes.
    const COUNT: usize;
}

/// A trait representing a lexer rule used for token matching.
/// Each rule can be converted into an index and defines a total count of rules
/// along with a designated end rule.
pub trait LexerRule: Copy + Debug + Into<usize> {
    /// Total number of lexer rules.
    const COUNT: usize;

    /// The end rule indicating termination of the lexing process.
    const END: Self;
}

/// Defines the data and configuration used by a lexer.
/// Provides access to lexer modes, rules, DFA data, and lookup utilities.
///
/// The struct implementing this trait is automatically generated by
/// **parlex-gen**'s lexer generator **Alex**.
pub trait LexerData {
    /// The mode type used by the lexer.
    type LexerMode: LexerMode;

    /// The rule type used by the lexer.
    type LexerRule: LexerRule;

    /// Returns the starting lexer mode.
    fn start_mode() -> Self::LexerMode;

    /// Returns the DFA state transition data as raw bytes.
    fn dfa_bytes() -> &'static [u8];

    /// Returns optional DFA offset data.
    fn dfa_offsets() -> &'static [usize] {
        &[]
    }

    /// Maps a mode and pattern ID to a lexer rule.
    fn lookup(mode: Self::LexerMode, pattern_id: usize) -> Self::LexerRule;
}

/// Defines the core lexer interface responsible for processing input streams and producing tokens.
///
/// The struct implementing this trait manages lexer state, performs rule-based matching, and handles
/// transitions between lexer modes. It also provides utility methods for token accumulation and buffer
/// management.
pub trait LexerDriver {
    /// The lexer data providing DFA tables and rule definitions.
    /// Must be provided by the implementer and must be the struct generated by
    /// **parlex-gen**’s lexer generator, **Alex**.
    type LexerData: LexerData;

    /// The token type produced by the lexer.
    /// Must be provided by the implementor.
    type Token: Token;

    /// Lexer type
    type Lexer;

    /// User context type
    type Context;

    /// Driver error type
    type Error;

    /// Executes a lexer action based on the matched rule, lexer and driver contexts
    /// Must be provided by the implementor.
    fn action(
        &mut self,
        lexer: &mut Self::Lexer,
        context: &mut Self::Context,
        rule: <Self::LexerData as LexerData>::LexerRule,
    ) -> Result<(), Self::Error>;
}

/// Statistics collected by the lexer during processing.
///
/// Tracks basic metrics about lexer activity and performance.
/// Typically generated and updated internally by the **parlex-gen**
/// lexer generator (**Alex**).
#[derive(Debug, Clone, Default)]
pub struct LexerStats {
    /// The number of characters that were unread or pushed back into the stream.
    pub unreads: usize,

    /// The total number of characters processed by the lexer.
    pub chars: usize,

    /// The number of successful token matches produced.
    pub matches: usize,
}

/// Core lexer implementation and execution engine.
///
/// The [`Lexer`] drives the DFA automata, manages input and buffering, and
/// invokes the [`LexerDriver`]'s `action` callback to produce tokens and
/// handle mode transitions.
///
/// It maintains all runtime state for lexical analysis, including the current
/// input position, active mode, and internal buffers.
///
/// # Type Parameters
///
/// - `I`: Input source that yields bytes via [`TryNextWithContext`].
/// - `D`: The [`LexerDriver`] implementation that defines how tokens are
///   produced and modes are managed.
///
/// [`LexerDriver`]: crate::LexerDriver
/// [`TryNextWithContext`]: crate::TryNextWithContext
pub struct Lexer<I, D>
where
    I: TryNextWithContext<Item = u8, Context = D::Context>,
    D: LexerDriver<Lexer = Lexer<I, D>>,
{
    /// Lexer driver that handles token emission and mode transitions.
    /// Temporarily taken out of the lexer during `action` invocations.
    driver: Option<Box<D>>,

    /// The input stream being processed by the lexer.
    input: I,

    /// The current lexer mode, defined by the driver’s [`LexerData`].
    mode: <D::LexerData as LexerData>::LexerMode,

    /// The compiled deterministic finite automata (DFA) tables used for lexing.
    #[cfg(not(test))]
    dfas: Vec<dense::DFA<&'static [u32]>>,

    /// The compiled deterministic finite automata (DFA) tables used for lexing (test mode).
    #[cfg(test)]
    dfas: Vec<dense::DFA<Vec<u32>>>,

    /// Stack of bytes that have been unread or pushed back into the stream.
    unread: Vec<u8>,

    /// Indicates whether the lexer is currently accumulating token text.
    accum_flag: bool,

    /// Primary buffer used to accumulate the characters of the current token.
    buffer: Vec<u8>,

    /// Signals that the end of input has been reached.
    end_flag: bool,

    /// A queue containing tokens that have been recognized but not yet consumed.
    /// Acts as the output buffer for tokens emitted by the lexer.
    tokens: VecDeque<D::Token>,

    /// The current line number in the input, used for diagnostics.
    line_no: usize,

    /// Statistics collected during lexing.
    stats: LexerStats,
}

/// Implementation of [`Lexer`] methods.
///
/// This `impl` defines the core construction and execution logic for the
/// lexer engine. It includes routines to create new lexer instances, load
/// DFA tables, drive automata transitions, and invoke the driver’s `action`
/// callback to emit tokens and handle mode changes.
///
/// [`Lexer`]: crate::Lexer
impl<I, D> Lexer<I, D>
where
    I: TryNextWithContext<Item = u8, Context = D::Context>,
    D: LexerDriver<Lexer = Lexer<I, D>>,
{
    /// Constructs a new [`Lexer`] from the given input source and driver.
    ///
    /// Initializes DFA tables from `<D::LexerData as LexerData>::dfa_bytes()`,
    /// sets the lexer to its starting mode, and prepares all internal buffers,
    /// flags, and counters for tokenization.
    ///
    /// # Parameters
    /// - `input`: The input stream providing bytes to be lexed.
    /// - `driver`: The [`LexerDriver`] implementation responsible for handling
    ///   token emission and mode transitions.
    ///
    /// # Returns
    /// A fully initialized [`Lexer`] ready to begin lexical analysis.
    ///
    /// # Errors
    /// Returns a [`LexerError::DfaDeserialize`] if any DFA fails to deserialize
    /// via [`dense::DFA::from_bytes`].
    ///
    /// [`LexerDriver`]: crate::LexerDriver
    pub fn try_new(input: I, driver: D) -> Result<Self, LexerError<I::Error, D::Error>> {
        Ok(Self {
            driver: Some(Box::new(driver)),
            input,
            mode: <D as LexerDriver>::LexerData::start_mode(),
            dfas: Self::restore_dfas()?,
            unread: Vec::new(),
            accum_flag: false,
            buffer: Vec::new(),
            end_flag: false,
            tokens: VecDeque::new(),
            line_no: 1,
            stats: LexerStats::default(),
        })
    }

    #[cfg(not(test))]
    fn restore_dfas()
    -> Result<Vec<dense::DFA<&'static [u32]>>, regex_automata::util::wire::DeserializeError> {
        let mut dfas = Vec::new();
        let dfa_bytes = <D as LexerDriver>::LexerData::dfa_bytes();
        let mut offset = 0;
        for _ in 0..<D::LexerData as LexerData>::LexerMode::COUNT {
            let (dfa, len) = dense::DFA::from_bytes(&dfa_bytes[offset..])?;
            dfas.push(dfa);
            offset += len;
        }
        Ok(dfas)
    }

    #[cfg(test)]
    fn restore_dfas() -> Result<Vec<dense::DFA<Vec<u32>>>, dense::BuildError> {
        let mut dfas = Vec::new();
        let dfa = dense::DFA::new_many::<&str>(&[])?;
        dfas.push(dfa);
        Ok(dfas)
    }

    /// Switches the lexer to a different mode.
    #[inline]
    pub fn begin(&mut self, mode: <D::LexerData as LexerData>::LexerMode) {
        self.mode = mode;
    }

    /// Returns the current mode.
    #[inline]
    pub fn mode(&self) -> <D::LexerData as LexerData>::LexerMode {
        self.mode
    }

    /// Returns the current line number.
    #[inline]
    pub fn line_no(&self) -> usize {
        self.line_no
    }

    /// Increments line number.
    #[inline]
    pub fn inc_line_no(&mut self) {
        self.line_no += 1;
    }

    /// Switches on accumulation of bytes into the buffer.
    #[inline]
    pub fn accum(&mut self) {
        self.accum_flag = true;
    }

    /// Emits a token into the output queue.
    #[inline]
    pub fn yield_token(&mut self, token: D::Token) {
        self.tokens.push_back(token);
    }

    /// Clears the current buffer and disables accumulation.
    #[inline]
    pub fn clear(&mut self) {
        self.accum_flag = false;
        self.buffer.clear();
    }

    /// Takes accumulated bytes from the main buffer.
    #[inline]
    pub fn take_bytes(&mut self) -> Vec<u8> {
        self.accum_flag = false;
        mem::take(&mut self.buffer)
    }

    /// Takes accumulated bytes from the main buffer and converts them into a UTF-8 string.
    #[inline]
    pub fn take_str(&mut self) -> Result<String, std::string::FromUtf8Error> {
        let bytes = self.take_bytes();
        let s = std::string::String::from_utf8(bytes)?;
        Ok(s.into())
    }

    /// Retrieves the next byte to be processed by the lexer.
    ///
    /// This helper function first checks the `unread` stack for any bytes that
    /// were previously pushed back into the stream. If the stack is non-empty,
    /// the most recently unread byte is returned.
    ///
    /// If the stack is empty, it reads the next byte from the input source via
    /// [`TryNextWithContext::try_next_with_context`], passing along the mutable
    /// `context`. Each successful read increments the character counter in
    /// [`LexerStats`].
    ///
    /// # Parameters
    /// - `context`: Mutable reference to the input’s context, forwarded to the
    ///   input source when reading new bytes.
    /// - `unread`: Stack of bytes that have been pushed back into the stream.
    /// - `input`: The input source implementing [`TryNextWithContext`].
    /// - `stats`: Lexer statistics structure; its `chars` field is incremented
    ///   after each successfully read byte.
    ///
    /// # Returns
    /// - `Ok(Some(byte))` if a byte is available (either from `unread` or input).
    /// - `Ok(None)` if the end of input has been reached.
    ///
    /// # Errors
    /// Returns a [`LexerError::Input`] if the underlying input source reports
    /// an error during byte retrieval.
    ///
    /// [`TryNextWithContext`]: crate::TryNextWithContext
    /// [`LexerStats`]: crate::LexerStats
    #[inline]
    fn get_next_byte(
        context: &mut I::Context,
        unread: &mut Vec<u8>,
        input: &mut I,
        stats: &mut LexerStats,
    ) -> Result<Option<u8>, LexerError<I::Error, D::Error>> {
        match unread.pop() {
            Some(b) => Ok(Some(b)),
            None => {
                let b = input
                    .try_next_with_context(context)
                    .map_err(LexerError::Input)?;
                stats.chars += 1;
                Ok(b)
            }
        }
    }

    /// This helper temporarily takes ownership of the driver from the [`Lexer`]
    /// to satisfy Rust’s borrowing rules, allowing the driver to receive a
    /// mutable reference to the lexer during the callback without causing
    /// aliasing.
    ///
    /// The driver is removed from `self`, the callback is executed, and then
    /// the driver is reinserted once the action completes. This ensures safe,
    /// exclusive access to both the lexer and the driver during token handling.
    ///
    /// # Parameters
    /// - `context`: Mutable reference to the driver's context, forwarded to
    ///   the [`LexerDriver::action`] method.
    /// - `rule`: The matched lexer rule, as defined by the driver's
    ///   [`LexerData`].
    ///
    /// # Errors
    /// - Returns [`LexerError::MissingDriver`] if the driver has already been
    ///   taken or was not initialized.
    /// - Returns [`LexerError::Driver`] if the driver’s `action` method fails.
    ///
    /// [`Lexer`]: crate::Lexer
    /// [`action`]: crate::LexerDriver::action
    /// [`LexerDriver::action`]: crate::LexerDriver::action
    /// [`LexerData`]: crate::LexerData
    fn action(
        &mut self,
        context: &mut D::Context,
        rule: <D::LexerData as LexerData>::LexerRule,
    ) -> Result<(), LexerError<I::Error, D::Error>> {
        let mut driver = self
            .driver
            .take()
            .ok_or_else(|| LexerError::MissingDriver)?;
        driver
            .action(self, context, rule)
            .map_err(LexerError::Driver)?;
        self.driver = Some(driver);
        Ok(())
    }

    /// Attempts to match the next token pattern using the active DFA.
    ///
    /// This method drives the current DFA to consume bytes from the input,
    /// retrieving them through the [`get_next_byte`] helper. It advances over
    /// the stream as needed, updating internal buffers, lexer mode, and
    /// statistics until a pattern is recognized or no further transitions
    /// are possible.
    ///
    /// The input is read through [`TryNextWithContext`], using the provided
    /// `context` to pass any required state to the input source, as mediated
    /// by [`get_next_byte`].
    ///
    /// # Returns
    /// - `Ok(Some(id))`: A pattern with the given [`PatternID`] was matched.
    /// - `Ok(None)`: No valid pattern was matched at the current position.
    /// - `Err(...)`: A [`LexerError`] occurred during DFA execution or input
    ///   retrieval.
    ///
    /// [`get_next_byte`]: Self::get_next_byte
    /// [`TryNextWithContext`]: crate::TryNextWithContext
    fn try_match(
        &mut self,
        context: &mut I::Context,
    ) -> Result<Option<PatternID>, LexerError<I::Error, D::Error>> {
        self.stats.matches += 1;
        if !self.accum_flag {
            self.buffer.clear();
        }
        let dfa = &self.dfas[self.mode.into()];
        let mut state = dfa.start_state_forward(&RegexInput::new(&[]).anchored(Anchored::Yes))?;
        log::trace!(
            "START: mode={}, s={}",
            Into::<usize>::into(self.mode),
            state.as_usize()
        );
        let mut last_match = None;
        let mut i = 0;

        loop {
            let b =
                Self::get_next_byte(context, &mut self.unread, &mut self.input, &mut self.stats)?;
            match b {
                Some(b) => {
                    self.buffer.push(b);
                    state = dfa.next_state(state, b);
                    if dfa.is_special_state(state) {
                        if dfa.is_match_state(state) {
                            log::trace!(
                                "MATCH: i={}, b={:?}, n={}, p={}, s={}",
                                i,
                                b as char,
                                dfa.match_len(state),
                                dfa.match_pattern(state, 0).as_usize(),
                                state.as_usize()
                            );
                            last_match = Some(HalfMatch::new(dfa.match_pattern(state, 0), i));
                        } else if dfa.is_dead_state(state) || dfa.is_quit_state(state) {
                            if dfa.is_dead_state(state) {
                                log::trace!(
                                    "DEAD: i={}, b={:?}, s={}",
                                    i,
                                    b as char,
                                    state.as_usize()
                                );
                            } else {
                                log::trace!(
                                    "QUIT: i={}, b={:?}, s={}",
                                    i,
                                    b as char,
                                    state.as_usize()
                                );
                            }
                            match last_match {
                                Some(m) => {
                                    for _ in 0..i - m.offset() + 1 {
                                        match self.buffer.pop() {
                                            Some(x) => self.unread.push(x),
                                            None => return Err(LexerError::Overpop),
                                        }
                                    }
                                    return Ok(Some(m.pattern()));
                                }
                                None => {
                                    return Err(LexerError::BadByte(b));
                                }
                            }
                        }
                    } else {
                        log::trace!(
                            "OTHER: i={}, b={:?}, s={}; match={}, dead={}, quit={}, start={}, accel={}",
                            i,
                            b as char,
                            state.as_usize(),
                            dfa.is_match_state(state),
                            dfa.is_dead_state(state),
                            dfa.is_quit_state(state),
                            dfa.is_start_state(state),
                            dfa.is_accel_state(state),
                        );
                    }
                }
                None => break,
            }
            i = i + 1;
        }
        state = dfa.next_eoi_state(state);
        if dfa.is_match_state(state) {
            last_match = Some(HalfMatch::new(dfa.match_pattern(state, 0), i));
        }
        match last_match {
            Some(m) => {
                for _ in 0..i - m.offset() {
                    self.stats.unreads += 1;
                    match self.buffer.pop() {
                        Some(x) => self.unread.push(x),
                        None => return Err(LexerError::Overpop),
                    }
                }
                return Ok(Some(m.pattern()));
            }
            None => {
                return Ok(None);
            }
        }
    }
}

/// [`TryNextWithContext`] implementation for [`Lexer`].
///
/// Provides context-aware advancement of the lexer state.
///
/// [`TryNextWithContext`]: crate::TryNextWithContext
impl<I, D> TryNextWithContext for Lexer<I, D>
where
    I: TryNextWithContext<Item = u8, Context = D::Context>,
    D: LexerDriver<Lexer = Lexer<I, D>>,
{
    type Item = D::Token;
    type Error = LexerError<I::Error, D::Error>;
    type Context = D::Context;

    /// Advances the lexer to produce the next token.
    ///
    /// Behavior:
    /// - If a token is already queued, returns it immediately.
    /// - Otherwise, repeatedly calls [`try_match`] to obtain a matched pattern,
    ///   looks up the corresponding rule, and invokes the driver’s [`action`]
    ///   callback (which may enqueue one or more tokens). Returns the first
    ///   enqueued token, if any.
    /// - On exhaustion (no further matches), sets `end_flag`, invokes the
    ///   driver’s `END` rule via [`action`], then returns any remaining token
    ///   or `None` if none were produced.
    ///
    /// The `context` is forwarded to both input reads (via [`get_next_byte`]
    /// through [`try_match`]) and the driver’s [`action`] callback. During
    /// `action`, the driver is temporarily taken out of the lexer to satisfy
    /// Rust’s borrowing rules.
    ///
    /// Returns `Ok(Some(token))`, `Ok(None)` at end of input, or an error if
    /// input, DFA execution, or the driver fails.
    ///
    /// [`try_match`]: Self::try_match
    /// [`get_next_byte`]: Self::get_next_byte
    /// [`action`]: crate::LexerDriver::action
    #[inline]
    fn try_next_with_context(
        &mut self,
        context: &mut Self::Context,
    ) -> Result<Option<Self::Item>, Self::Error> {
        if let Some(t) = self.tokens.pop_front() {
            return Ok(Some(t));
        }

        if self.end_flag {
            return Ok(None);
        }

        while let Some(pattern) = self.try_match(context)? {
            let mode = self.mode;
            let rule = <D as LexerDriver>::LexerData::lookup(mode, pattern.as_usize());
            log::trace!(
                "MATCHED: LexerMode: {:?}, LexerRule: {:?}, Pattern: {}, Buffer: {:?}",
                mode,
                rule,
                pattern.as_usize(),
                match str::from_utf8(&self.buffer) {
                    Ok(s) => s.to_string(),
                    Err(_) => hex::encode(&self.buffer),
                }
            );

            self.action(context, rule)?;

            if let Some(t) = self.tokens.pop_front() {
                return Ok(Some(t));
            }
        }
        self.end_flag = true;

        self.action(context, <D::LexerData as LexerData>::LexerRule::END)?;

        if let Some(t) = self.tokens.pop_front() {
            return Ok(Some(t));
        } else {
            return Ok(None);
        }
    }
}

/// Unit tests for [`Lexer`] and related components.
#[cfg(test)]
mod tests {
    use crate::lexer::{Lexer, LexerData, LexerDriver, LexerError, LexerMode, LexerRule, Token};
    use smartstring::alias::String;
    use std::fmt::Debug;
    use try_next::TryNextWithContext;

    include!(concat!(
        env!("CARGO_MANIFEST_DIR"),
        "/src/test_lexer_data.rs"
    ));

    /// Initializes the test logger to enable log output during tests.
    fn init_logger() {
        let _ = env_logger::builder().is_test(true).try_init();
    }

    #[derive(Debug, Clone)]
    struct XToken {
        token_id: usize,
        line_no: usize,
    }
    impl Token for XToken {
        type TokenID = usize;

        fn token_id(&self) -> Self::TokenID {
            self.token_id
        }
        fn line_no(&self) -> usize {
            self.line_no
        }
    }

    struct XLexerDriver<I> {
        _marker: std::marker::PhantomData<I>,
    }

    impl<I> LexerDriver for XLexerDriver<I>
    where
        I: TryNextWithContext<Item = u8, Context = String>,
    {
        type LexerData = LexData;
        type Token = XToken;
        type Lexer = Lexer<I, Self>;
        type Error = std::convert::Infallible;
        type Context = I::Context;

        fn action(
            &mut self,
            lexer: &mut Self::Lexer,
            context: &mut Self::Context,
            rule: <Self::LexerData as LexerData>::LexerRule,
        ) -> Result<(), Self::Error> {
            let token = match rule {
                Rule::Empty => unreachable!(),
                Rule::A => XToken {
                    token_id: 1,
                    line_no: lexer.line_no(),
                }, // <Expr> .
                Rule::End => XToken {
                    token_id: 2,
                    line_no: lexer.line_no(),
                },
            };
            lexer.yield_token(token);
            context.push('l');
            Ok(())
        }
    }
    struct XLexer<I>
    where
        I: TryNextWithContext<Item = u8, Context = String>,
    {
        lexer: Lexer<I, XLexerDriver<I>>,
    }

    impl<I> XLexer<I>
    where
        I: TryNextWithContext<Item = u8, Context = String>,
    {
        fn try_new(
            input: I,
        ) -> Result<
            Self,
            LexerError<<I as TryNextWithContext>::Error, <XLexerDriver<I> as LexerDriver>::Error>,
        > {
            let driver = XLexerDriver {
                _marker: std::marker::PhantomData,
            };
            let lexer = Lexer::try_new(input, driver)?;
            Ok(Self { lexer })
        }
    }
    impl<I> TryNextWithContext for XLexer<I>
    where
        I: TryNextWithContext<Item = u8, Context = String>,
    {
        type Item = XToken;
        type Error =
            LexerError<<I as TryNextWithContext>::Error, <XLexerDriver<I> as LexerDriver>::Error>;
        type Context = I::Context;

        fn try_next_with_context(
            &mut self,
            context: &mut I::Context,
        ) -> Result<Option<XToken>, <Self as TryNextWithContext>::Error> {
            context.push('a');
            self.lexer.try_next_with_context(context)
        }
    }

    struct Empty {}
    impl TryNextWithContext for Empty {
        type Item = u8;
        type Error = std::convert::Infallible;
        type Context = String;
        fn try_next_with_context(
            &mut self,
            context: &mut Self::Context,
        ) -> Result<Option<Self::Item>, Self::Error> {
            context.push('e');
            Ok(None)
        }
    }

    /// Tests that an empty lexer can be created and run without errors.
    #[test]
    fn empty_lexer() {
        init_logger();
        let mut context = String::new();
        let input = Empty {};
        let mut lexer = XLexer::try_new(input).unwrap();
        while let Some(t) = lexer.try_next_with_context(&mut context).unwrap() {
            dbg!(&t);
        }
        assert_eq!(context, "aela");
    }
}
