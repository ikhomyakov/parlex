//! Core lexical analysis traits and context.
//!
//! This module defines the key traits used by lexers generated by the [`alex`]
//! generator in the [`parlex-gen`](https://crates.io/crates/parlex-gen) crate. It
//! provides the `Token`, `LexerMode`, `LexerRule`, `LexerData` and `Lexer` traits,
//! along with the [`LexerCtx`] type that maintains state while scanning input. A
//! generated lexer will implement the [`Lexer`] trait and use these definitions to
//! drive the DFA transitions, buffer accumulation and token production. See the
//! README for more details.
//!
//! [`alex`]: https://crates.io/crates/parlex-gen

use anyhow::{Result, bail};
use regex_automata::{
    Anchored, HalfMatch, Input,
    dfa::{Automaton, dense},
    util::primitives::PatternID,
};
use smartstring::alias::String;
use std::collections::VecDeque;
use std::fmt::Debug;
use std::iter::FusedIterator;
use std::mem;

/// A trait representing a token in lexical analysis or parsing.
///
/// Defines the minimal interface for objects treated as tokens, providing
/// a token identifier and its line position in the source.
pub trait Token: Clone + Debug {
    /// The type used to represent token identifiers.
    type TokenID: Into<usize>;

    /// Returns the identifier of this token.
    fn token_id(&self) -> Self::TokenID;

    /// Returns the line number where this token appears in the source.
    fn line_no(&self) -> usize;
}

/// A trait representing a lexer mode used during lexical analysis.
/// Each mode can be converted into an index and defines a total count of modes.
pub trait LexerMode: Copy + Debug + Into<usize> {
    /// Total number of lexer modes.
    const COUNT: usize;
}

/// A trait representing a lexer rule used for token matching.
/// Each rule can be converted into an index and defines a total count of rules
/// along with a designated end rule.
pub trait LexerRule: Copy + Debug + Into<usize> {
    /// Total number of lexer rules.
    const COUNT: usize;

    /// The end rule indicating termination of the lexing process.
    const END: Self;
}

/// Defines the data and configuration used by a lexer.
/// Provides access to lexer modes, rules, DFA data, and lookup utilities.
///
/// The struct implementing this trait is automatically generated by
/// **parlex-gen**'s lexer generator **Alex**.
pub trait LexerData {
    /// The mode type used by the lexer.
    type LexerMode: LexerMode;

    /// The rule type used by the lexer.
    type LexerRule: LexerRule;

    /// Returns the starting lexer mode.
    fn start_mode() -> Self::LexerMode;

    /// Returns the DFA state transition data as raw bytes.
    fn dfa_bytes() -> &'static [u8];

    /// Returns optional DFA offset data.
    fn dfa_offsets() -> &'static [usize] {
        &[]
    }

    /// Maps a mode and pattern ID to a lexer rule.
    fn lookup(mode: Self::LexerMode, pattern_id: usize) -> Self::LexerRule;
}

/// Defines the core lexer interface responsible for processing input streams and producing tokens.
///
/// The struct implementing this trait manages lexer state, performs rule-based matching, and handles
/// transitions between lexer modes. It also provides utility methods for token accumulation and buffer
/// management.
///
/// # Type Parameters
/// - `U`: User-defined data passed to lexer actions.
pub trait Lexer<U> {
    /// The input stream type producing bytes for lexing.
    type Input: FusedIterator<Item = u8>;

    /// The lexer data providing DFA tables and rule definitions.
    /// Must be provided by the implementer and must be the struct generated by
    /// **parlex-gen**’s lexer generator, **Alex**.
    type LexerData: LexerData;

    /// The token type produced by the lexer.
    /// Must be provided by the implementor.
    type Token: Token;

    /// Returns a shared reference to the lexer context.
    /// Must be provided by the implementor and return `&LexerCtx`.
    fn ctx(&self) -> &LexerCtx<Self::Input, Self::LexerData, Self::Token>;

    /// Returns a mutable reference to the lexer context.
    /// Must be provided by the implementor and return `&mut LexerCtx`.
    fn ctx_mut(&mut self) -> &mut LexerCtx<Self::Input, Self::LexerData, Self::Token>;

    /// Executes a lexer action based on the matched rule and user data.
    /// Must be provided by the implementor.
    fn action(
        &mut self,
        user_data: &mut U,
        rule: <Self::LexerData as LexerData>::LexerRule,
    ) -> Result<()>;

    /// Returns current lexer statistics.
    fn stats(&self) -> LexerStats {
        self.ctx().stats.clone()
    }

    /// Collects all tokens from the input until exhaustion.
    fn try_collect(&mut self, user_data: &mut U) -> Result<Vec<Self::Token>> {
        let mut ts = Vec::new();
        while let Some(t) = self.try_next(user_data)? {
            ts.push(t);
        }
        Ok(ts)
    }

    /// Attempts to fetch the next token from the input stream.
    #[inline]
    fn try_next(&mut self, user_data: &mut U) -> Result<Option<Self::Token>> {
        if let Some(t) = self.ctx_mut().tokens.pop_front() {
            return Ok(Some(t));
        }

        if self.ctx().end_flag {
            return Ok(None);
        }

        while let Some(pattern) = self.ctx_mut().try_match()? {
            let mode = self.ctx().mode;
            let rule = <Self as Lexer<U>>::LexerData::lookup(mode, pattern.as_usize());
            log::trace!(
                "MATCHED: LexerMode: {:?}, LexerRule: {:?}, Pattern: {}, Buffer: {:?}, Buffer2: {:?}",
                mode,
                rule,
                pattern.as_usize(),
                match str::from_utf8(&self.ctx().buffer) {
                    Ok(s) => s.to_string(),
                    Err(_) => hex::encode(&self.ctx().buffer),
                },
                match str::from_utf8(&self.ctx().buffer2) {
                    Ok(s) => s.to_string(),
                    Err(_) => hex::encode(&self.ctx().buffer2),
                },
            );

            self.action(user_data, rule)?;

            if let Some(t) = self.ctx_mut().tokens.pop_front() {
                return Ok(Some(t));
            }
        }
        self.ctx_mut().end_flag = true;

        self.action(user_data, <Self::LexerData as LexerData>::LexerRule::END)?;

        if let Some(t) = self.ctx_mut().tokens.pop_front() {
            return Ok(Some(t));
        } else {
            return Ok(None);
        }
    }

    /// Switches on accumulation of bytes into the buffer.
    #[inline]
    fn accum(&mut self) {
        self.ctx_mut().accum_flag = true;
    }

    /// Switches the lexer to a different mode.
    #[inline]
    fn begin(&mut self, mode: <Self::LexerData as LexerData>::LexerMode) {
        self.ctx_mut().mode = mode;
    }

    /// Emits a token into the output queue.
    #[inline]
    fn yield_token(&mut self, token: Self::Token) {
        self.ctx_mut().tokens.push_back(token);
    }

    /// Clears the current buffer and disables accumulation.
    #[inline]
    fn clear(&mut self) {
        self.ctx_mut().accum_flag = false;
        self.ctx_mut().buffer.clear();
    }

    /// Takes accumulated bytes from the main buffer.
    #[inline]
    fn take_bytes(&mut self) -> Vec<u8> {
        self.ctx_mut().accum_flag = false;
        mem::take(&mut self.ctx_mut().buffer)
    }

    /// Takes accumulated bytes from the secondary buffer.
    #[inline]
    fn take_bytes2(&mut self) -> Vec<u8> {
        self.ctx_mut().accum_flag = false;
        self.ctx_mut().buffer.clear();
        mem::take(&mut self.ctx_mut().buffer2)
    }

    /// Takes accumulated bytes from the main buffer and converts them into a UTF-8 string.
    #[inline]
    fn take_str(&mut self) -> Result<String> {
        let bytes = self.take_bytes();
        let s = std::string::String::from_utf8(bytes)?;
        Ok(s.into())
    }

    /// Takes accumulated bytes from the secondary buffer and converts them into a UTF-8 string.
    #[inline]
    fn take_str2(&mut self) -> Result<String> {
        let bytes = self.take_bytes2();
        let s = std::string::String::from_utf8(bytes)?;
        Ok(s.into())
    }

    /// Extends the secondary buffer with the contents of the main buffer.
    #[inline]
    fn extend_buffer2_with_buffer(&mut self) {
        let ctx = self.ctx_mut();
        ctx.buffer2.extend(&ctx.buffer);
    }
}

/// Statistics collected by the lexer during processing.
///
/// Tracks basic metrics about lexer activity and performance.
/// Typically generated and updated internally by the **parlex-gen**
/// lexer generator (**Alex**).
#[derive(Debug, Clone, Default)]
pub struct LexerStats {
    /// The number of characters that were unread or pushed back into the stream.
    pub unreads: usize,

    /// The total number of characters processed by the lexer.
    pub chars: usize,

    /// The number of successful token matches produced.
    pub matches: usize,
}

/// Core lexer execution context.
///
/// `LexerCtx` manages the entire runtime state of the lexer, including its input stream,
/// DFA tables, buffers, and recognized tokens. It is responsible for coordinating
/// lexing operations, tracking position and mode, and maintaining internal statistics.
///
/// # Type Parameters
/// - `I`: The input source, typically an iterator over bytes.
/// - `D`: The lexer definition, which implements the [`LexerData`] trait.
///   This type is usually generated by the **parlex-gen** lexer generator (**Alex**).
/// - `T`: The token type produced by the lexer.
pub struct LexerCtx<I, D, T>
where
    D: LexerData,
{
    /// The current lexer mode, defined by the lexer data type `D`.
    pub mode: D::LexerMode,

    /// The compiled deterministic finite automata (DFA) tables used for lexing.
    dfas: Vec<dense::DFA<&'static [u32]>>,

    /// The input stream being processed by the lexer.
    input: I,

    /// A stack of bytes that have been unread or pushed back into the stream.
    unread: Vec<u8>,

    /// Indicates whether the lexer is currently accumulating token text.
    pub accum_flag: bool,

    /// Primary buffer used to accumulate the characters of the current token.
    pub buffer: Vec<u8>,

    /// Secondary buffer available to the user, for example, for constructing more complex tokens.
    pub buffer2: Vec<u8>,

    /// Signals that the end of input has been reached.
    pub end_flag: bool,

    /// A queue containing tokens that have been recognized but not yet consumed.
    /// Acts as the output buffer for tokens emitted by the lexer.
    tokens: VecDeque<T>,

    /// The current line number in the input, used for diagnostics.
    pub line_no: usize,

    /// Statistics collected during lexing.
    stats: LexerStats,
}

/// Implementation of [`LexerCtx`] methods.
///
/// This `impl` defines the primary construction and matching logic for the lexer context.
/// It provides methods to initialize a new lexer context instance, load DFA tables,
/// and perform token matching over the input stream.
///
/// # Type Parameters
/// - `I`: The input source, which must be a [`FusedIterator`] yielding bytes.
/// - `D`: The lexer definition type implementing the [`LexerData`] trait.
///   This type is usually generated by the **parlex-gen** lexer generator (**Alex**).
/// - `T`: The token type implementing the [`Token`] trait.
impl<I, D, T> LexerCtx<I, D, T>
where
    I: FusedIterator<Item = u8>,
    D: LexerData,
    T: Token,
{
    /// Constructs a new [`LexerCtx`] from the given input.
    ///
    /// This initializes DFA tables from `D::dfa_bytes()`, sets the lexer to its
    /// starting mode, and prepares all buffers and counters for processing.
    ///
    /// # Parameters
    /// - `input`: A fused iterator of bytes serving as the lexer’s input stream.
    ///
    /// # Returns
    /// A fully initialized lexer context, or an error if the DFA tables cannot be
    /// deserialized.
    ///
    /// # Errors
    /// Returns an error if any DFA fails to deserialize via [`dense::DFA::from_bytes`].
    pub fn try_new(input: I) -> Result<Self> {
        let mut dfas = Vec::new();
        let dfa_bytes = D::dfa_bytes();
        let mut offset = 0;
        for _ in 0..D::LexerMode::COUNT {
            let (dfa, len) = dense::DFA::from_bytes(&dfa_bytes[offset..])?;
            dfas.push(dfa);
            offset += len;
        }

        Ok(Self {
            mode: D::start_mode(),
            dfas,
            input,
            unread: Vec::new(),
            accum_flag: false,
            buffer: Vec::new(),
            buffer2: Vec::new(),
            end_flag: false,
            tokens: VecDeque::new(),
            line_no: 1,
            stats: LexerStats::default(),
        })
    }

    /// Attempts to match the next token using the current DFA and input state.
    ///
    /// Advances over the input as needed and updates internal buffers, mode, and
    /// statistics. The result indicates whether a valid pattern was matched.
    ///
    /// # Returns
    /// - `Ok(Some(id))`: A pattern was matched successfully.
    /// - `Ok(None)`: No match was found at the current position.
    /// - `Err(...)`: An error occurred during DFA execution or input handling.
    fn try_match(&mut self) -> Result<Option<PatternID>> {
        self.stats.matches += 1;
        if !self.accum_flag {
            self.buffer.clear();
        }
        let dfa = &self.dfas[self.mode.into()];
        let mut state = dfa.start_state_forward(&Input::new(&[]).anchored(Anchored::Yes))?;
        log::trace!(
            "START: mode={}, s={}",
            Into::<usize>::into(self.mode),
            state.as_usize()
        );
        let mut last_match = None;
        let mut i = 0;

        loop {
            match self.unread.pop().or_else(|| {
                self.stats.chars += 1;
                self.input.next()
            }) {
                Some(b) => {
                    self.buffer.push(b);
                    state = dfa.next_state(state, b);
                    if dfa.is_special_state(state) {
                        if dfa.is_match_state(state) {
                            log::trace!(
                                "MATCH: i={}, b={:?}, n={}, p={}, s={}",
                                i,
                                b as char,
                                dfa.match_len(state),
                                dfa.match_pattern(state, 0).as_usize(),
                                state.as_usize()
                            );
                            last_match = Some(HalfMatch::new(dfa.match_pattern(state, 0), i));
                        } else if dfa.is_dead_state(state) || dfa.is_quit_state(state) {
                            if dfa.is_dead_state(state) {
                                log::trace!(
                                    "DEAD: i={}, b={:?}, s={}",
                                    i,
                                    b as char,
                                    state.as_usize()
                                );
                            } else {
                                log::trace!(
                                    "QUIT: i={}, b={:?}, s={}",
                                    i,
                                    b as char,
                                    state.as_usize()
                                );
                            }
                            match last_match {
                                Some(m) => {
                                    for _ in 0..i - m.offset() + 1 {
                                        match self.buffer.pop() {
                                            Some(x) => self.unread.push(x),
                                            None => bail!("Overpop!"),
                                        }
                                    }
                                    return Ok(Some(m.pattern()));
                                }
                                None => {
                                    bail!("Bad byte {:?}", b);
                                }
                            }
                        }
                    } else {
                        log::trace!(
                            "OTHER: i={}, b={:?}, s={}; match={}, dead={}, quit={}, start={}, accel={}",
                            i,
                            b as char,
                            state.as_usize(),
                            dfa.is_match_state(state),
                            dfa.is_dead_state(state),
                            dfa.is_quit_state(state),
                            dfa.is_start_state(state),
                            dfa.is_accel_state(state),
                        );
                    }
                }
                None => break,
            }
            i = i + 1;
        }
        state = dfa.next_eoi_state(state);
        if dfa.is_match_state(state) {
            last_match = Some(HalfMatch::new(dfa.match_pattern(state, 0), i));
        }
        match last_match {
            Some(m) => {
                for _ in 0..i - m.offset() {
                    self.stats.unreads += 1;
                    match self.buffer.pop() {
                        Some(x) => self.unread.push(x),
                        None => bail!("Overpop!"),
                    }
                }
                return Ok(Some(m.pattern()));
            }
            None => {
                return Ok(None);
            }
        }
    }
}

/// Unit tests for [`LexerCtx`] and related components.
#[cfg(test)]
mod tests {
    use super::*;

    /// Initializes the test logger to enable log output during tests.
    fn init_logger() {
        let _ = env_logger::builder().is_test(true).try_init();
    }

    #[derive(Debug, Clone, Copy)]
    struct XLexerMode;
    impl LexerMode for XLexerMode {
        const COUNT: usize = 0;
    }
    impl Into<usize> for XLexerMode {
        fn into(self) -> usize {
            0
        }
    }

    #[derive(Debug, Clone, Copy)]
    struct XLexerRule;
    impl LexerRule for XLexerRule {
        const COUNT: usize = 0;
        const END: Self = Self;
    }
    impl Into<usize> for XLexerRule {
        fn into(self) -> usize {
            0
        }
    }

    #[derive(Debug, Clone, Copy, Default)]
    struct XToken {
        token_id: usize,
        line_no: usize,
    }
    impl Token for XToken {
        type TokenID = usize;

        fn token_id(&self) -> Self::TokenID {
            self.token_id
        }
        fn line_no(&self) -> usize {
            self.line_no
        }
    }

    struct XLexerData {}
    impl LexerData for XLexerData {
        type LexerMode = XLexerMode;
        type LexerRule = XLexerRule;

        fn start_mode() -> Self::LexerMode {
            XLexerMode
        }
        fn dfa_bytes() -> &'static [u8] {
            &[]
        }

        #[inline]
        fn lookup(_mode: Self::LexerMode, _pattern_id: usize) -> Self::LexerRule {
            XLexerRule
        }
    }

    struct XLexer<I>
    where
        I: FusedIterator<Item = u8>,
    {
        ctx: LexerCtx<I, <Self as Lexer<()>>::LexerData, <Self as Lexer<()>>::Token>,
    }

    impl<I> XLexer<I>
    where
        I: FusedIterator<Item = u8>,
    {
        fn try_new(input: I) -> Result<Self> {
            let mut ctx = LexerCtx::try_new(input)?;
            ctx.end_flag = true;
            Ok(Self { ctx })
        }
    }

    impl<I> Lexer<()> for XLexer<I>
    where
        I: FusedIterator<Item = u8>,
    {
        type Input = I;
        type LexerData = XLexerData;
        type Token = XToken;

        fn ctx(&self) -> &LexerCtx<Self::Input, Self::LexerData, Self::Token> {
            &self.ctx
        }
        fn ctx_mut(&mut self) -> &mut LexerCtx<Self::Input, Self::LexerData, Self::Token> {
            &mut self.ctx
        }

        fn action(
            &mut self,
            _user_data: &mut (),
            _rule: <Self::LexerData as LexerData>::LexerRule,
        ) -> Result<()> {
            self.yield_token(XToken {
                token_id: 0,
                line_no: 0,
            });
            Ok(())
        }
    }

    /// Tests that an empty lexer can be created and run without errors.
    #[test]
    fn empty_lexer() {
        init_logger();
        let s = "hello";
        let mut lexer = XLexer::try_new(s.bytes().fuse()).unwrap();
        while let Some(t) = lexer.try_next(&mut ()).unwrap() {
            dbg!(t);
        }
    }
}
