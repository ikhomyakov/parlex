//! Core parsing traits and actions.
//!
//! This module defines the traits and data structures used by SLR parsers generated by
//! the [`aslr`] tool in the [`parlex-gen`](https://crates.io/crates/parlex-gen) crate.
//! It introduces the [`Parser`] trait as the entry point for parsing streams of
//! tokens produced by a lexer. It also contains the [`ParserAction`] enum and
//! associated trait families that control reductions, state transitions and ambiguity
//! resolution. See the README for an overview.
//!
//! [`aslr`]: https://crates.io/crates/parlex-gen

use crate::{Lexer, Token};
use anyhow::{Result, anyhow, bail};
use smartstring::alias::String;
use std::fmt::Debug;

/// Parser action used by the parsing automaton.
///
/// Represents the next operation the parser should perform, such as shifting,
/// reducing, accepting, etc. Parameterized by identifiers for parser states,
/// productions, and ambiguities.
///
/// # Type Parameters
/// - `S`: Parser state identifier type implementing [`ParserStateID`].
/// - `P`: Production identifier type implementing [`ParserProdID`].
/// - `A`: Ambiguity identifier type implementing [`ParserAmbigID`].
#[derive(Clone, Copy, Debug, PartialEq, Eq)]
pub enum ParserAction<S, P, A>
where
    S: ParserStateID,
    P: ParserProdID,
    A: ParserAmbigID,
{
    /// Indicates a parsing error.
    Error,

    /// Signals successful parsing (accepting the input).
    Accept,

    /// Shifts the next token and transitions to the given parser state.
    Shift(S),

    /// Reduces by the given production rule.
    Reduce(P),

    /// Handles an ambiguity represented by the given ambiguity ID.
    Ambig(A),

    /// Performs a `goto` transition to the given parser state.
    Goto(S),
}

/// A trait representing an identifier for a parser state.
///
/// Each parser state corresponds to a position in the parsing automaton.
/// The type implementing this trait is typically generated by
/// **parlex-gen**’s parser generator **ASLR**.
pub trait ParserStateID: Copy + Debug + Eq + Into<usize> {
    /// The total number of parser states.
    const COUNT: usize;
}

/// A trait representing an identifier for an ambiguity in the grammar.
///
/// Ambiguities occur when multiple parses are possible for the same input span.
/// The type implementing this trait is typically generated by
/// **parlex-gen**’s parser generator **ASLR**.
pub trait ParserAmbigID: Copy + Debug + Eq + Into<usize> {
    /// The total number of ambiguity identifiers.
    const COUNT: usize;
}

/// A trait representing an identifier for a grammar production rule.
///
/// A production defines how tokens and nonterminals combine to form higher-level
/// syntactic constructs. The type implementing this trait is typically generated by
/// **parlex-gen**’s parser generator **ASLR**.
pub trait ParserProdID: Copy + Debug + Eq + Into<usize> {
    /// The associated token identifier type.
    type TokenID: ParserTokenID;

    /// The total number of production rules.
    const COUNT: usize;

    /// Returns the label (name) of this production.
    fn label(&self) -> &'static str;

    /// Returns the token ID of the left-hand side (LHS) nonterminal.
    fn lhs_token_id(&self) -> Self::TokenID;

    /// Returns the number of symbols on the right-hand side (RHS) of the production.
    fn size(&self) -> usize;
}

/// A trait representing an identifier for a terminal or nonterminal token in the grammar.
///
/// Token IDs represent both grammar terminals and nonterminals used by the parser.
/// The type implementing this trait is typically generated by
/// **parlex-gen**’s parser generator **ASLR**.
pub trait ParserTokenID: Copy + Debug + Eq + Into<usize> {
    /// The number of nonterminal symbols in the grammar.
    const COUNT_NONTERMINALS: usize;

    /// The number of terminal symbols in the grammar.
    const COUNT_TERMINALS: usize;

    /// The total number of tokens: terminals + nonterminals + 1 (error).
    const COUNT: usize;

    /// Returns the label (name) of this token.
    fn label(&self) -> &'static str;
}

/// Helper alias for the parser’s [`ParserAction`] type.
type Action<P, U> = ParserAction<
    <<P as Parser<U>>::ParserData as ParserData>::StateID,
    <<P as Parser<U>>::ParserData as ParserData>::ProdID,
    <<P as Parser<U>>::ParserData as ParserData>::AmbigID,
>;

/// Defines the data and configuration used by a parser.
/// Provides access to parser states, productions, ambiguities, and lookup tables.
///
/// The struct implementing this trait is automatically generated by
/// **parlex-gen**’s parser generator **ASLR**.
pub trait ParserData {
    /// Identifier type for parser states.
    type StateID: ParserStateID;

    /// Identifier type for grammar ambiguities.
    type AmbigID: ParserAmbigID;

    /// Identifier type for grammar tokens (terminals and nonterminals).
    type TokenID: ParserTokenID;

    /// Identifier type for grammar productions.
    type ProdID: ParserProdID;

    /// Returns the starting parser state.
    fn start_state() -> Self::StateID;

    /// Looks up the parser action for the given state and input token.
    fn lookup(
        state_id: Self::StateID,
        token_id: Self::TokenID,
    ) -> ParserAction<Self::StateID, Self::ProdID, Self::AmbigID>;

    /// Looks up the ambiguity table entry for the given ambiguity ID.
    ///
    /// Returns a reference to a static two-element array representing the
    /// ambiguity record. Each entry corresponds to a possible parser action.
    ///
    /// One action is typically a **Shift** and the other a **Reduce**,
    /// forming a classic *shift/reduce conflict*. Such conflicts can often be resolved
    /// using operator precedence and associativity rules.
    ///
    /// In contrast, *reduce/reduce* conflicts are generally not resolvable and
    /// usually indicate that the grammar should be refactored.
    fn lookup_ambig(
        ambig_id: Self::AmbigID,
    ) -> &'static [ParserAction<Self::StateID, Self::ProdID, Self::AmbigID>; 2];
}

/// Defines the core parser interface responsible for driving the parsing process.
///
/// The struct implementing this trait coordinates parser state, interacts with the lexer,
/// resolves ambiguities, and applies reductions based on the parser tables. It also provides
/// utility methods for inspecting and manipulating the parse stack.
///
/// # Type Parameters
/// - `U`: User-defined data passed to parser actions.
pub trait Parser<U> {
    /// The lexer used by this parser.
    /// Must be provided by the implementor and produce tokens compatible with `ParserData::TokenID`.
    type Lexer: Lexer<U, Token: Token<TokenID = <Self::ParserData as ParserData>::TokenID>>;

    /// The parser data providing states, productions, tokens, and lookup tables.
    /// Must be provided by the implementor and be the struct generated by
    /// **parlex-gen**’s parser generator, **ASLR**.
    type ParserData: ParserData;

    /// Returns a shared reference to the parser context.
    /// Must be provided by the implementor and return `&ParserCtx`.
    fn ctx(&self) -> &ParserCtx<Self::Lexer, Self::ParserData, U>;

    /// Returns a mutable reference to the parser context.
    /// Must be provided by the implementor and return `&mut ParserCtx`.
    fn ctx_mut(&mut self) -> &mut ParserCtx<Self::Lexer, Self::ParserData, U>;

    /// Resolves an ambiguity using user-defined logic (e.g., precedence/associativity).
    /// Must be provided by the implementor.
    fn resolve_ambiguity(
        &mut self,
        user_data: &mut U,
        ambig: <Self::ParserData as ParserData>::AmbigID,
        tok2: &<Self::Lexer as Lexer<U>>::Token,
    ) -> Result<Action<Self, U>>;

    /// Performs a reduction for the specified production.
    /// Must be provided by the implementor.
    fn reduce(
        &mut self,
        user_data: &mut U,
        prod_id: <Self::ParserData as ParserData>::ProdID,
        token: &<Self::Lexer as Lexer<U>>::Token,
    ) -> Result<()>;

    /// Returns current parser statistics.
    fn stats(&self) -> ParserStats {
        self.ctx().stats.clone()
    }

    /// Returns a reference to a token counted from the end of the stack.
    /// `0` = last (top), `1` = second last, etc.
    ///
    /// # Panics
    /// Panics if `index` ≥ the number of tokens on the stack.
    #[inline]
    fn tokens_peek<'a>(&'a self, index: usize) -> &'a <Self::Lexer as Lexer<U>>::Token
    where
        U: 'a,
    {
        let n = self.ctx().tokens.len();
        &self.ctx().tokens[n - 1 - index]
    }

    /// Returns a mutable reference to a token counted from the end of the stack.
    /// `0` = last (top), `1` = second last, etc.
    ///
    /// # Panics
    /// Panics if `index` ≥ the number of tokens on the stack.
    #[inline]
    fn tokens_mut_peek<'a>(&'a mut self, index: usize) -> &'a mut <Self::Lexer as Lexer<U>>::Token
    where
        U: 'a,
    {
        let n = self.ctx().tokens.len();
        &mut self.ctx_mut().tokens[n - 1 - index]
    }

    /// Pops and returns the last (top) token from the stack.
    ///
    /// # Errors
    /// Returns an error if the stack is empty.
    #[inline]
    fn tokens_pop(&mut self) -> Result<<Self::Lexer as Lexer<U>>::Token> {
        self.ctx_mut()
            .tokens
            .pop()
            .ok_or_else(|| anyhow!("stack underflow"))
    }

    /// Pushes a token onto the stack.
    #[inline]
    fn tokens_push(&mut self, token: <Self::Lexer as Lexer<U>>::Token) {
        self.ctx_mut().tokens.push(token);
    }

    /// Traces the current parser state and token stack for debugging.
    /// Formats the stack with states and tokens, marking the incoming token with `<-`.
    fn dump_state(&self, incoming: &<Self::Lexer as Lexer<U>>::Token) {
        let mut output = String::new();
        if !self.ctx().states.is_empty() {
            for (i, (token, state)) in self
                .ctx()
                .tokens
                .iter()
                .chain(std::iter::once(incoming))
                .zip(self.ctx().states.iter())
                .enumerate()
            {
                output.push_str(&format!(
                    "<{:?}>  {}{:?}  ",
                    state,
                    if i == self.ctx().states.len() - 1 {
                        "<-  "
                    } else {
                        ""
                    },
                    token,
                ));
            }
            log::trace!("{}", output);
        } else {
            log::trace!("<>");
        }
    }

    /// Attempts to collect all tokens until exhaustion.
    fn try_collect(&mut self, user_data: &mut U) -> Result<Vec<<Self::Lexer as Lexer<U>>::Token>> {
        let mut ts = Vec::new();
        while let Some(t) = self.try_next(user_data)? {
            ts.push(t);
        }
        Ok(ts)
    }

    /// Attempts to parse the input and produce the next reduced (accepted) token.
    #[inline]
    fn try_next(&mut self, user_data: &mut U) -> Result<Option<<Self::Lexer as Lexer<U>>::Token>> {
        self.ctx_mut().states.clear();
        self.ctx_mut().tokens.clear();
        self.ctx_mut().stats.tokens += 1;
        let mut token = match self.ctx_mut().lexer.try_next(user_data)? {
            Some(t) => t,
            None => {
                return Ok(None);
            }
        };
        let mut state = <Self as Parser<U>>::ParserData::start_state();
        self.ctx_mut().states.push(state);
        if log::log_enabled!(log::Level::Trace) {
            self.dump_state(&token);
        }
        loop {
            let action = match <Self as Parser<U>>::ParserData::lookup(state, token.token_id()) {
                Action::<Self, U>::Ambig(ambig) => {
                    log::trace!("Ambig {:?}", ambig);
                    let action = self.resolve_ambiguity(user_data, ambig, &mut token)?;
                    self.ctx_mut().stats.ambigs += 1;
                    action
                }
                action => action,
            };
            match action {
                Action::<Self, U>::Shift(new_state) => {
                    log::trace!("Shift {:?}", new_state);
                    self.ctx_mut().tokens.push(token);
                    state = new_state;
                    self.ctx_mut().states.push(state);
                    self.ctx_mut().stats.tokens += 1;
                    token = match self.ctx_mut().lexer.try_next(user_data)? {
                        Some(t) => t,
                        None => bail!("unexpected end of stream"),
                    };
                    self.ctx_mut().stats.shifts += 1;
                }

                Action::<Self, U>::Reduce(prod) => {
                    log::trace!("Reduce {:?}({})", prod, Into::<usize>::into(prod));
                    self.reduce(user_data, prod, &token)?;
                    let n = self.ctx().states.len() - prod.size();
                    self.ctx_mut().states.truncate(n);
                    state = self.ctx().states[self.ctx().states.len() - 1];
                    let lhs_id = self.ctx().tokens[self.ctx().tokens.len() - 1].token_id();
                    let Action::<Self, U>::Goto(new_state) =
                        <Self as Parser<U>>::ParserData::lookup(state, lhs_id)
                    else {
                        bail!("expected Action::Goto");
                    };
                    state = new_state;
                    self.ctx_mut().states.push(state);
                    self.ctx_mut().stats.reductions += 1;
                }

                Action::<Self, U>::Accept => {
                    log::trace!("Accept");
                    assert!(self.ctx().tokens.len() == 1);
                    let token = self.tokens_pop()?;
                    return Ok(Some(token));
                }

                Action::<Self, U>::Error => {
                    bail!("Error on token {:?}", token)
                }

                Action::<Self, U>::Ambig(_) | Action::<Self, U>::Goto(_) => unreachable!(),
            }

            if log::log_enabled!(log::Level::Trace) {
                self.dump_state(&token);
            }
        }
    }
}

/// Statistics collected during the parsing process.
///
/// Tracks basic metrics about parser activity and performance.
#[derive(Debug, Clone, Default)]
pub struct ParserStats {
    /// The total number of tokens processed by the parser.
    pub tokens: usize,

    /// The number of shift actions performed.
    pub shifts: usize,

    /// The number of reduce actions performed.
    pub reductions: usize,

    /// The number of ambiguities encountered.
    pub ambigs: usize,
}

/// Core parser execution context.
///
/// `ParserCtx` manages the entire runtime state of the parser, including its lexer,
/// parser states, token stack, and statistics. It is responsible for coordinating
/// parsing operations, maintaining the parser stack, and collecting performance metrics.
///
/// # Type Parameters
/// - `L`: The lexer type, which implements the [`Lexer`] trait.
/// - `D`: The parser data definition, which implements the [`ParserData`] trait.
///   This type is usually generated by the **parlex-gen** parser generator (**ASLR**).
/// - `U`: User-defined data passed to parser and lexer actions.
pub struct ParserCtx<L, D, U>
where
    L: Lexer<U>,
    D: ParserData,
{
    /// The lexer instance used to produce tokens.
    pub lexer: L,

    /// The stack of tokens currently held by the parser.
    pub tokens: Vec<L::Token>,

    /// The stack of parser state identifiers.
    pub states: Vec<D::StateID>,

    /// Statistics collected during parsing.
    pub stats: ParserStats,
}

/// Implementation of [`ParserCtx`] methods.
///
/// This `impl` defines the core construction logic for the parser context.
/// It provides methods to initialize a new parser context instance and set up
/// the runtime environment for parsing, including token and state stacks
/// and parser statistics.
///
/// # Type Parameters
/// - `L`: The lexer type implementing the [`Lexer`] trait.
/// - `D`: The parser data type implementing the [`ParserData`] trait.
///   This type is usually generated by the **parlex-gen** parser generator (**ASLR**).
/// - `U`: User-defined data passed to parser and lexer actions.
impl<L, D, U> ParserCtx<L, D, U>
where
    L: Lexer<U>,
    D: ParserData,
{
    /// Constructs a new [`ParserCtx`] from the given lexer.
    ///
    /// This initializes empty token and state stacks and resets parser statistics.
    ///
    /// # Parameters
    /// - `lexer`: The lexer instance used to produce tokens for this parser.
    ///
    /// # Returns
    /// A fully initialized parser context ready for use.
    pub fn new(lexer: L) -> Self {
        Self {
            lexer,
            tokens: Vec::new(),
            states: Vec::new(),
            stats: ParserStats::default(),
        }
    }
}

/// Unit tests for [`ParserCtx`] and related components.
#[cfg(test)]
mod tests {
    use super::Action;
    use crate::lexer::{Lexer, LexerCtx, LexerData, LexerMode, LexerRule, LexerStats, Token};
    use crate::parser::{
        Parser, ParserAction, ParserAmbigID, ParserCtx, ParserData, ParserProdID, ParserStateID,
        ParserStats, ParserTokenID,
    };
    use anyhow::{Result, anyhow, bail};
    use regex_automata::PatternID;
    use smartstring::alias::String;
    use std::fmt::Debug;
    use std::iter::FusedIterator;

    /// Initializes the test logger to enable log output during tests.
    fn init_logger() {
        let _ = env_logger::builder().is_test(true).try_init();
    }

    #[derive(Debug, Clone, Copy)]
    struct XLexerMode;
    impl LexerMode for XLexerMode {
        const COUNT: usize = 0;
    }
    impl Into<usize> for XLexerMode {
        fn into(self) -> usize {
            0
        }
    }

    #[derive(Debug, Clone, Copy)]
    struct XLexerRule;
    impl LexerRule for XLexerRule {
        const COUNT: usize = 0;
        const END: Self = Self;
    }
    impl Into<usize> for XLexerRule {
        fn into(self) -> usize {
            0
        }
    }

    #[derive(Debug, Clone, Copy, PartialEq, Eq, Default)]
    struct TokenID(usize);

    impl From<TokenID> for usize {
        fn from(token_id: TokenID) -> Self {
            token_id.0
        }
    }

    impl ParserTokenID for TokenID {
        const COUNT_NONTERMINALS: usize = 5;
        const COUNT_TERMINALS: usize = 10;
        const COUNT: usize = 15;

        fn label(&self) -> &'static str {
            ""
        }
    }

    #[derive(Debug, Clone, Copy, PartialEq, Eq, Default)]
    struct StateID(usize);

    impl From<StateID> for usize {
        fn from(state_id: StateID) -> Self {
            state_id.0
        }
    }

    impl ParserStateID for StateID {
        const COUNT: usize = 45;
    }

    #[derive(Debug, Clone, Copy, PartialEq, Eq)]
    struct ProdID(usize);

    impl From<ProdID> for usize {
        fn from(prod_id: ProdID) -> Self {
            prod_id.0
        }
    }

    impl ParserProdID for ProdID {
        type TokenID = TokenID;

        const COUNT: usize = 24;

        fn label(&self) -> &'static str {
            ""
        }
        fn lhs_token_id(&self) -> Self::TokenID {
            TokenID(0)
        }
        fn size(&self) -> usize {
            0
        }
    }

    #[derive(Debug, Clone, Copy, PartialEq, Eq)]
    struct AmbigID(usize);

    impl From<AmbigID> for usize {
        fn from(token_id: AmbigID) -> Self {
            token_id.0
        }
    }

    impl ParserAmbigID for AmbigID {
        const COUNT: usize = 1500;
    }

    #[derive(Debug, Clone, Copy, Default)]
    struct XToken {
        token_id: TokenID,
        line_no: usize,
    }
    impl Token for XToken {
        type TokenID = TokenID;

        fn token_id(&self) -> Self::TokenID {
            self.token_id
        }
        fn line_no(&self) -> usize {
            self.line_no
        }
    }

    struct XLexerData {}
    impl LexerData for XLexerData {
        type LexerMode = XLexerMode;
        type LexerRule = XLexerRule;

        fn start_mode() -> Self::LexerMode {
            XLexerMode
        }
        fn dfa_bytes() -> &'static [u8] {
            &[]
        }

        #[inline]
        fn lookup(_mode: Self::LexerMode, _pattern_id: usize) -> Self::LexerRule {
            XLexerRule
        }
    }

    struct XLexer<I>
    where
        I: FusedIterator<Item = u8>,
    {
        ctx: LexerCtx<I, <Self as Lexer<()>>::LexerData, <Self as Lexer<()>>::Token>,
    }

    impl<I> XLexer<I>
    where
        I: FusedIterator<Item = u8>,
    {
        fn try_new(input: I) -> Result<Self> {
            let mut ctx = LexerCtx::try_new(input)?;
            ctx.end_flag = true;
            Ok(Self { ctx })
        }
    }

    impl<I> Lexer<()> for XLexer<I>
    where
        I: FusedIterator<Item = u8>,
    {
        type Input = I;
        type LexerData = XLexerData;
        type Token = XToken;

        fn ctx(&self) -> &LexerCtx<Self::Input, Self::LexerData, Self::Token> {
            &self.ctx
        }
        fn ctx_mut(&mut self) -> &mut LexerCtx<Self::Input, Self::LexerData, Self::Token> {
            &mut self.ctx
        }

        fn action(
            &mut self,
            user_data: &mut (),
            _rule: <Self::LexerData as LexerData>::LexerRule,
        ) -> Result<()> {
            self.yield_token(XToken {
                token_id: TokenID(0),
                line_no: 0,
            });
            Ok(())
        }
    }

    const AMBIG: &'static [ParserAction<StateID, ProdID, AmbigID>; 2] = &[
        ParserAction::Shift(StateID(0)),
        ParserAction::Reduce(ProdID(0)),
    ];

    struct XParserData {}
    impl ParserData for XParserData {
        type StateID = StateID;
        type AmbigID = AmbigID;
        type TokenID = TokenID;
        type ProdID = ProdID;

        fn start_state() -> Self::StateID {
            StateID::default()
        }

        fn lookup(
            state_id: Self::StateID,
            token_id: Self::TokenID,
        ) -> ParserAction<Self::StateID, Self::ProdID, Self::AmbigID> {
            ParserAction::Accept
        }

        fn lookup_ambig(
            ambig_id: Self::AmbigID,
        ) -> &'static [ParserAction<Self::StateID, Self::ProdID, Self::AmbigID>; 2] {
            AMBIG
        }
    }

    struct XParser<I>
    where
        I: FusedIterator<Item = u8>,
    {
        ctx: ParserCtx<XLexer<I>, <Self as Parser<()>>::ParserData, ()>,
    }

    impl<I> XParser<I>
    where
        I: FusedIterator<Item = u8>,
    {
        fn try_new(input: I) -> Result<Self> {
            let lexer = XLexer::try_new(input)?;
            let ctx = ParserCtx::new(lexer);
            Ok(Self { ctx })
        }
    }

    impl<I> Parser<()> for XParser<I>
    where
        I: FusedIterator<Item = u8>,
    {
        type Lexer = XLexer<I>;
        type ParserData = XParserData;

        fn ctx(&self) -> &ParserCtx<Self::Lexer, Self::ParserData, ()> {
            &self.ctx
        }
        fn ctx_mut(&mut self) -> &mut ParserCtx<Self::Lexer, Self::ParserData, ()> {
            &mut self.ctx
        }

        fn resolve_ambiguity(
            &mut self,
            user_data: &mut (),
            ambig: <Self::ParserData as ParserData>::AmbigID,
            tok2: &<Self::Lexer as Lexer<()>>::Token,
        ) -> Result<Action<Self, ()>> {
            Ok(Action::<Self, ()>::Shift(StateID::default()))
        }

        fn reduce(
            &mut self,
            user_data: &mut (),
            prod_id: <Self::ParserData as ParserData>::ProdID,
            token: &<Self::Lexer as Lexer<()>>::Token,
        ) -> Result<()> {
            Ok(())
        }
    }

    /// Tests that an empty parser can be created and run without errors.
    #[test]
    fn empty_parser() {
        init_logger();
        let s = "hello";
        let mut parser = XParser::try_new(s.bytes().fuse()).unwrap();
        while let Some(t) = parser.try_next(&mut ()).unwrap() {
            dbg!(t);
        }
    }
}
